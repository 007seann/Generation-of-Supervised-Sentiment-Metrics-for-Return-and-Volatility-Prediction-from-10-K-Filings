{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1844763d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "from ratelimit import limits, sleep_and_retry\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "from collections import Counter\n",
    "import re\n",
    "import csv\n",
    "from nltk.tokenize import word_tokenize\n",
    "import concurrent.futures  # Import the concurrent.futures module\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83010b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ratelimit\n",
      "  Using cached ratelimit-2.2.1-py3-none-any.whl\n",
      "Installing collected packages: ratelimit\n",
      "Successfully installed ratelimit-2.2.1\n"
     ]
    }
   ],
   "source": [
    "!pip3 install ratelimit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa0e736",
   "metadata": {},
   "source": [
    "functions for TXT format reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24761b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#try to find page numbers in txt format reports\n",
    "def contains_only_numbers(row):\n",
    "    for cell in row:\n",
    "        if not re.match(r'^\\d+$', cell):\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "617b1bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#try to find headings that consist of all capital letters\n",
    "def capital_sentence_detect(content):\n",
    "    sentences = content.split('.')\n",
    "    modified_content =''\n",
    "    for sentence in sentences:\n",
    "        if sentence.isupper():\n",
    "            sentence = f\"[heading]{sentence}[/heading]\"\n",
    "        modified_content += sentence + \" \"\n",
    "        \n",
    "    return modified_content.strip()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec2003f",
   "metadata": {},
   "source": [
    "functions for HTML format reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b6515ee",
   "metadata": {},
   "outputs": [],
   "source": [
    " #find page break tags and replace with keyword 'split_of_pages'\n",
    "def find_page_break_tags(soup):\n",
    "    #first format of page tag\n",
    "    page_break_tags = soup.find_all('hr', color=\"#999999\")\n",
    "    #second format of page tag\n",
    "    if len(page_break_tags) < 6:\n",
    "        page_break_tags = soup.find_all('hr')\n",
    "    #third format of page tag - find <div> elements with style 'page-break-after: always'\n",
    "    if len(page_break_tags) < 6:\n",
    "        page_break_tags = soup.find_all('div', style=lambda value: (value \n",
    "                                                            and re.search(r'page-break-after\\s*:\\s*always', value, re.IGNORECASE) \n",
    "                                                            and 'position:relative' not in value))                        \n",
    "    #replace page tags with split_of_pages\n",
    "    for tag in page_break_tags:\n",
    "        tag.replace_with('split_of_pages')\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "862d6819",
   "metadata": {},
   "outputs": [],
   "source": [
    "#find heading tags and add [heading][/heading]\n",
    "def find_headings(soup):\n",
    "    #heading tag with html style bold or special color\n",
    "    for tag in (soup.find_all(style=lambda value: (value and (\n",
    "                'font-weight:bold' in value.lower() or\n",
    "                'font-weight: bold' in value.lower() or\n",
    "                'font-weight:700' in value.lower() or\n",
    "                'font-weight: 700' in value.lower() or \n",
    "                'color:#0068b5' in value.lower() or\n",
    "                'font-size:22pt' in value.lower()\n",
    "    )))):\n",
    "        content = tag.get_text()\n",
    "        tag.string = f\"\\n[heading]{content}[/heading]\\n\"\n",
    "                    \n",
    "                    \n",
    "    #heading tag b         \n",
    "    for tag in soup.find_all('b'):\n",
    "        content = tag.get_text()\n",
    "        tag.string = f\"\\n[heading]{content}[/heading]\\n\"\n",
    "    \n",
    "    #heading tag strong  \n",
    "    for tag in (soup.find_all('strong')):\n",
    "        content = tag.get_text()\n",
    "        tag.string = f\"\\n[heading]{content}[/heading]\\n\"\n",
    "                \n",
    "    #heading tag with html style underline\n",
    "    for tag in (soup.find_all(style=lambda value: (value and (\n",
    "        'text-decoration:underline' in value.lower() or\n",
    "        'text-decoration: underline' in value.lower())))):\n",
    "        content = tag.get_text()\n",
    "        tag.string = f\"\\n[heading]{content}[/heading]\\n\"\n",
    "        \n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ccf341c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#find heading tags and add [heading][/heading]\n",
    "def find_headings_with_italic(soup):\n",
    "    #heading tag with html style bold or special color\n",
    "    for tag in (soup.find_all(style=lambda value: (value and (\n",
    "                'font-weight:bold' in value.lower() or\n",
    "                'font-weight: bold' in value.lower() or\n",
    "                'font-weight:700' in value.lower() or\n",
    "                'font-weight: 700' in value.lower() or \n",
    "                'color:#0068b5' in value.lower() or\n",
    "                'font-size:22pt' in value.lower()\n",
    "    )))):\n",
    "        content = tag.get_text()\n",
    "        tag.string = f\"\\n[heading]{content}[/heading]\\n\"\n",
    "                    \n",
    "                    \n",
    "    #heading tag b                   \n",
    "    for tag in soup.find_all('b'):\n",
    "        content = tag.get_text()\n",
    "        tag.string = f\"\\n[heading]{content}[/heading]\\n\"\n",
    "    \n",
    "    #heading tag strong   \n",
    "    for tag in (soup.find_all('strong')):\n",
    "        content = tag.get_text()\n",
    "        tag.string = f\"\\n[heading]{content}[/heading]\\n\"\n",
    "    \n",
    "    #heading tag em             \n",
    "    for tag in (soup.find_all('em')):\n",
    "        content = tag.get_text()\n",
    "        tag.string = f\"\\n[heading]{content}[/heading]\\n\"\n",
    "    \n",
    "    #heading tag with html style underline               \n",
    "    for tag in (soup.find_all(style=lambda value: (value and (\n",
    "        'text-decoration:underline' in value.lower() or\n",
    "        'text-decoration: underline' in value.lower())))):\n",
    "        content = tag.get_text()\n",
    "        tag.string = f\"\\n[heading]{content}[/heading]\\n\"\n",
    "    \n",
    "    #heading tag with html style italic \n",
    "    for tag in soup.find_all(True, style=True):\n",
    "        style_value = tag.get('style')  # Get the style attribute value\n",
    "        if style_value and ('italic' in style_value.lower()):\n",
    "            content = tag.get_text()\n",
    "            tag.string = f\"\\n[heading]{content}[/heading]\\n\" \n",
    "    \n",
    "    #heading tag i   \n",
    "    for tag in (soup.find_all('i')):\n",
    "        content = tag.get_text()\n",
    "        tag.string = f\"\\n[heading]{content}[/heading]\\n\"\n",
    "        \n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6065b250",
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine nested heading tags e.g. [heading][heading] and [/heading][/heading]\n",
    "def combine_adjacent_headings(content):\n",
    "    while re.search(r'\\[heading\\]\\s*\\[heading\\]', content):\n",
    "        content = re.sub(r'\\[heading\\]\\s*\\[heading\\]', '[heading]', content)\n",
    "    while re.search(r'\\[/heading\\]\\s*\\[/heading\\]', content):\n",
    "        content = re.sub(r'\\[/heading\\]\\s*\\[/heading\\]', '[/heading]', content)\n",
    "    return content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a05f077",
   "metadata": {},
   "outputs": [],
   "source": [
    "#judge whether the elements near 'split of page' is page footer or not\n",
    "def remove_page_footer_by_line(text_list,position,min_occurrence,footer_type='text'):\n",
    "    # get the elements to be deleted by label 'split_of_pages'\n",
    "    if footer_type == 'length':\n",
    "        #if forward\n",
    "        if position<0:\n",
    "            delete_elements = [len(text_list[i +position]) for i in range(-position,len(text_list)) if text_list[i] == 'split_of_pages']\n",
    "            # get the content of elements whose occurrences are greater than occurrence\n",
    "            counter = Counter(delete_elements)\n",
    "            length_to_delete = [length for length, count in counter.items() if count >= min_occurrence]\n",
    "            # Iterate in reverse order and delete the corresponding element\n",
    "            for i in range(len(text_list) - 1, -position-1, -1):\n",
    "                if text_list[i] == 'split_of_pages' and len(text_list[i +position]) in length_to_delete:\n",
    "                    if text_list[i +position]!='split_of_pages':\n",
    "                        del text_list[i +position]\n",
    "        #if backward\n",
    "        if position>0:\n",
    "            delete_elements = [len(text_list[i +position]) for i in range(len(text_list)-position) if text_list[i] == 'split_of_pages']\n",
    "            # get the content of elements whose occurrences are greater than occurrence\n",
    "            counter = Counter(delete_elements)\n",
    "            length_to_delete = [length for length, count in counter.items() if count >= min_occurrence]\n",
    "            # Iterate in reverse order and delete the corresponding element\n",
    "            for i in range(len(text_list) -position-1 , -1, -1):\n",
    "                if text_list[i] == 'split_of_pages' and len(text_list[i +position]) in length_to_delete:\n",
    "                    if text_list[i +position]!='split_of_pages':\n",
    "                        del text_list[i +position]\n",
    "                        \n",
    "                        \n",
    "    if footer_type == 'text':\n",
    "        #if forward\n",
    "        if position<0:\n",
    "            delete_elements = [text_list[i +position] for i in range(-position,len(text_list)) if text_list[i] == 'split_of_pages']\n",
    "            # get the content of elements whose occurrences are greater than occurrence\n",
    "            counter = Counter(delete_elements)\n",
    "            texts_to_delete = [text for text, count in counter.items() if count >= min_occurrence]\n",
    "\n",
    "            # Iterate in reverse order and delete the corresponding element\n",
    "            for i in range(len(text_list) - 1, -position-1, -1):\n",
    "                if text_list[i] == 'split_of_pages' and text_list[i +position] in texts_to_delete:\n",
    "                    if text_list[i +position]!='split_of_pages':\n",
    "                        del text_list[i +position]\n",
    "        #if backward\n",
    "        if position>0:\n",
    "            delete_elements = [text_list[i +position] for i in range(len(text_list)-position) if text_list[i] == 'split_of_pages']\n",
    "            # get the content of elements whose occurrences are greater than occurrence\n",
    "            counter = Counter(delete_elements)\n",
    "            texts_to_delete = [text for text, count in counter.items() if count >= min_occurrence]\n",
    "\n",
    "            # Iterate in reverse order and delete the corresponding element\n",
    "            for i in range(len(text_list) -position-1 , -1, -1):\n",
    "                if text_list[i] == 'split_of_pages' and text_list[i +position] in texts_to_delete:\n",
    "                    if text_list[i +position]!='split_of_pages':\n",
    "                        del text_list[i +position]\n",
    "    return text_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "568339ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove page footers by parameters generated from common page footers\n",
    "def remove_page_footer(text_list):\n",
    "\n",
    "    # Remove page footer, the third element forward\n",
    "    text_list = remove_page_footer_by_line(text_list,-3,3,'text')\n",
    "\n",
    "    # Remove page footer, the second element forward\n",
    "    text_list = remove_page_footer_by_line(text_list,-2,3,'text')\n",
    "    \n",
    "    # Remove page number, the second element forward\n",
    "    text_list = remove_page_footer_by_line(text_list,-2,5,'length')\n",
    "    \n",
    "    # Remove page number, the first element forward\n",
    "    text_list = remove_page_footer_by_line(text_list,-1,5,'length')\n",
    "    \n",
    "    # Remove page footer, backward\n",
    "    for i in range (6,0,-1):\n",
    "        text_list = remove_page_footer_by_line(text_list,i,5,'text')\n",
    "    \n",
    "    return text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "62a2eda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the position of keyword, keyword must be in [heading][/heading] tags\n",
    "def find_heading_positions_with_keyword(content, keyword_list):\n",
    "    heading_positions = []\n",
    "    pattern = r'\\[heading\\](.*?)\\[/heading\\]'\n",
    "    headings = re.findall(pattern, content, re.DOTALL)\n",
    "    \n",
    "    for heading in headings:\n",
    "        if len(heading) < 100:\n",
    "            for keyword in keyword_list:\n",
    "                keyword = re.sub(r'\\W+', '', keyword).lower()\n",
    "                alphanumeric_text = re.sub(r'\\W+', '', heading).lower()\n",
    "                if keyword in alphanumeric_text:\n",
    "                    heading_start = content.find(f'[heading]{heading}[/heading]')\n",
    "                    heading_end = heading_start + len(f'[heading]{heading}[/heading]')\n",
    "                    heading_positions.append((heading_start, heading_end))\n",
    "            \n",
    "    return heading_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "21c2dd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the pairs of position that are most likely to be correct\n",
    "def find_max_difference_pair(first_heading_position, second_heading_position):\n",
    "    try:\n",
    "        max_difference = 0\n",
    "        max_difference_pair = None\n",
    "\n",
    "        for first_heading in first_heading_position:\n",
    "            valid_second_headings = [second_heading for second_heading in second_heading_position if second_heading[0] > first_heading[0]]\n",
    "            if not valid_second_headings:\n",
    "                continue\n",
    "            \n",
    "            closest_start = min(valid_second_headings, key=lambda x: x[0])\n",
    "            difference = closest_start[0] - first_heading[0]\n",
    "            \n",
    "            if difference > max_difference and difference > 1000:  # Limit difference to be larger than 1000\n",
    "                max_difference = difference\n",
    "                max_difference_pair = (first_heading[1], closest_start[0])\n",
    "\n",
    "        return max_difference_pair, None  # Return None for error status\n",
    "\n",
    "    except Exception as e:\n",
    "        return None, str(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e97deba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove headings that do not contain number or character\n",
    "def process_headings(match):\n",
    "    content = match.group(1)\n",
    "    if any(c.isalnum() for c in content):\n",
    "        return '[heading]'+content+'[/heading]'\n",
    "    else:\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "72791288",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the first heading, if it is 'item1A risk factors', remove heading\n",
    "def remove_heading_risk_factors(content):\n",
    "    pattern = r'\\[heading\\](.*?)\\[/heading\\]'\n",
    "    match = re.search(pattern, content, re.DOTALL)  # Find the first pair of [heading][/heading]\n",
    "    if match:\n",
    "        heading_content = match.group(1)\n",
    "        if 'riskfactors' in re.sub(r'\\W+', '', heading_content).lower():  # Check if 'riskfactors' is in the heading text\n",
    "            content = content.replace(match.group(0), '')  # Remove the entire [heading][/heading] pair\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6e25960c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove heading that contained '(Continued)' or '(continued)'\n",
    "def remove_heading_risk_factors_continued(content):\n",
    "    pattern = r'\\[heading\\](.*?)\\[/heading\\]'\n",
    "    matches = re.findall(pattern, content, re.DOTALL)\n",
    "    for match in matches:\n",
    "        heading_content = match\n",
    "        if re.search(r'\\([Cc]ontinued\\)', heading_content):  # Check if '(Continued)' or '(continued)' is in the heading text\n",
    "            content = content.replace(f'[heading]{match}[/heading]', '')  # Remove the entire [heading][/heading] pair\n",
    "    return content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c686453c",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "#find titles, split titles from headings\n",
    "def find_title(elements):\n",
    "    new_elements = []\n",
    "    i = 0\n",
    "    while i < len(elements):\n",
    "        element = elements[i]\n",
    "        if '[heading]' in element and '[/heading]' in element:\n",
    "            heading_text = re.search(r'\\[heading\\](.*?)\\[/heading\\]', element).group(1)\n",
    "            \n",
    "            words = re.findall(r'\\b\\w+\\b', heading_text)\n",
    "            non_stop_words = [word for word in words if word.lower() not in stop_words]\n",
    "            all_start_with_capital = all((word[0].isupper() or word[0].isdigit()) for word in non_stop_words) and len(non_stop_words)>=1\n",
    "            \n",
    "            if all_start_with_capital and i + 1 < len(elements) and '[heading]' in elements[i + 1]:\n",
    "                new_element = element.replace('[heading]', '[title]').replace('[/heading]', '[/title]')\n",
    "                new_elements.append(new_element)\n",
    "            else:\n",
    "                new_elements.append(element)\n",
    "        else:\n",
    "            new_elements.append(element)\n",
    "        i += 1\n",
    "    return new_elements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b0a4780b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split titles from headings\n",
    "def split_title_heading_content(content):\n",
    "    # Extract content before the first [heading] tag\n",
    "    before_first_heading_pattern = r'^(.*?)(?=\\[heading\\])'\n",
    "    before_first_heading_match = re.search(before_first_heading_pattern, content, re.DOTALL)\n",
    "\n",
    "    # Extract content between [heading] and [/heading] tags along with non-heading content\n",
    "    heading_content_pattern = r'(\\[heading\\].*?\\[/heading\\])\\s*(.*?)(?=\\[heading\\]|\\Z)'\n",
    "    heading_matches = re.findall(heading_content_pattern, content, re.DOTALL)\n",
    "\n",
    "    combined_contents = []\n",
    "    \n",
    "    # Add content before the first [heading] tag if found\n",
    "    if before_first_heading_match:\n",
    "        combined_contents.append(before_first_heading_match.group(1).strip())\n",
    "\n",
    "    for heading_content, non_heading_content in heading_matches:\n",
    "        combined_contents.append(heading_content)\n",
    "        if non_heading_content.strip():\n",
    "            combined_contents.append(non_heading_content)\n",
    "\n",
    "    # Process and join the content\n",
    "    content = find_title(combined_contents)  \n",
    "    content = ' '.join(content)\n",
    "    return content\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4978d813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regular expression pattern to match [heading] tags and their content\n",
    "\n",
    "def process_heading_pairs(match):\n",
    "    content = match.group(1)\n",
    "    word_count = len(word_tokenize(content))  # Count words using word_tokenize\n",
    "    if word_count < 10:\n",
    "        return content\n",
    "    else:\n",
    "        return f\"[/heading]{content}[heading]\"\n",
    "    \n",
    "def filter_content_between_heading_pairs(content):\n",
    "    # Replace the matched content using the process_heading function\n",
    "    pattern = r'\\[/heading\\](.*?)\\[heading\\]'\n",
    "    content = re.sub(r'\\s+', ' ', content)\n",
    "    content = re.sub(pattern, process_heading_pairs, content)\n",
    "    content = content.replace('[heading]','\\n\\n[heading]').replace('[/heading]','[/heading]\\n\\n')\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "78b8eec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove headings that are not likely to be heading\n",
    "def remove_short_heading(content):\n",
    "    pattern_heading = r'\\[heading\\](.*?)\\[/heading\\]'\n",
    "    # Replace short [heading][/heading] pairs with their content\n",
    "    matches = re.finditer(pattern_heading, content)\n",
    "    for match in matches:\n",
    "        heading_content = match.group(1)\n",
    "        words_in_heading = word_tokenize(heading_content)  # Tokenize words using NLTK\n",
    "        # Define a regex pattern to match only symbols\n",
    "        symbol_pattern = r'^[^\\w]+$'\n",
    "\n",
    "        # Filter out tokens that consist only of symbols using regex\n",
    "        words_in_heading = [word for word in words_in_heading if not re.match(symbol_pattern, word)]\n",
    "\n",
    "        # Check if the heading contains specific phrases\n",
    "        if ('FORWARD-LOOKING STATEMENTS' in heading_content.upper() or\n",
    "            'FORWARD-LOOKING STATEMENTS' in words_in_heading):\n",
    "            continue  # Skip filtering if the phrase is found\n",
    "\n",
    "        if len(words_in_heading) <=1:\n",
    "            full_pair = match.group(0)  # The entire [heading][/heading] pair\n",
    "            content = content.replace(full_pair, heading_content)\n",
    "    return content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1685fffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check and remove tags for headings that are followed by ','\n",
    "def replace_heading_with_comma(content):\n",
    "    pattern_heading = r'\\[heading\\](.*?)\\[/heading\\]'\n",
    "    matches = re.finditer(pattern_heading, content)\n",
    "    modified_content = content\n",
    "    \n",
    "    for match in matches:\n",
    "        heading_content = match.group(1)\n",
    "        next_chars_start = match.end()  # Position immediately after [/heading]\n",
    "        next_chars_end = next_chars_start + 3\n",
    "        \n",
    "        if ',' in content[next_chars_start:next_chars_end]:\n",
    "            to_replace = match.group(0)  # Entire [heading][/heading] pattern\n",
    "            modified_content = modified_content.replace(to_replace, heading_content)\n",
    "            \n",
    "    return modified_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7783330d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove headings that are not likely to be heading\n",
    "def filter_illegal_heading(content):\n",
    "    pattern_heading = r'\\[heading\\](.*?)\\[/heading\\]'\n",
    "    matches = re.finditer(pattern_heading, content)\n",
    "    modified_content = content\n",
    "    \n",
    "    for match in matches:\n",
    "        heading_content = match.group(1)\n",
    "        next_chars_start = match.end()  # Position immediately after [/heading]\n",
    "        \n",
    "        # Find the first character that's not a space or a newline\n",
    "        non_whitespace_char = re.search(r'[^\\s\\n]', content[next_chars_start:])\n",
    "        \n",
    "        if non_whitespace_char and non_whitespace_char.group(0) == '[':\n",
    "            # This is a nested [heading] tag, do nothing\n",
    "            pass\n",
    "        elif non_whitespace_char and non_whitespace_char.group(0).islower():\n",
    "            # Replace the illegal [heading][/heading] pattern with just the heading content\n",
    "            to_replace = match.group(0)\n",
    "            modified_content = modified_content.replace(to_replace, heading_content, 1)\n",
    "            \n",
    "    return modified_content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d3e7b4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#error log function\n",
    "def save_error_info(cik, file_name, error_csv_path):\n",
    "    error_info = [cik, file_name]\n",
    "    with open(error_csv_path, 'a', newline='', encoding='utf-8') as error_file:\n",
    "        error_writer = csv.writer(error_file)\n",
    "        error_writer.writerow(error_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "46ad30a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''without italic'''\n",
    "\n",
    "def process_files_for_cik(cik):    \n",
    "    read_folder = os.path.join(root_folder, cik)\n",
    "    save_folder_factor = os.path.join(root_folder_risk_factors, cik)\n",
    "    if not os.path.exists(save_folder_factor):\n",
    "        os.makedirs(save_folder_factor)\n",
    "    if read_folder == 'data/.DS_Store':\n",
    "            return\n",
    "    for file in os.listdir(read_folder):\n",
    "        read_path = os.path.join(read_folder, file)\n",
    "        if os.path.splitext(read_path)[1] == '.txt':\n",
    "            read_folder = os.path.join(root_folder, cik)\n",
    "\n",
    "            read_path = os.path.join(read_folder, file)\n",
    "            content = []\n",
    "            with open(read_path, 'r', encoding='utf-8') as input_txt:\n",
    "                for line in input_txt:\n",
    "                    row = line.strip().split()  # Assuming the file contains space-separated values\n",
    "                    if not contains_only_numbers(row):\n",
    "                        content.append(line)\n",
    "            content = ''.join(content)\n",
    "            content = re.sub(r'\\s+', ' ', content)\n",
    "            content = re.sub(r'\\n.*\\n<PAGE>', '', content)\n",
    "            content = re.sub(r'\\n.*\\n</TEXT>', '', content)\n",
    "            content = re.sub(r'<[^>]+>', '', content)\n",
    "            # Find positions of 'ITEM 1A. RISK FACTORS' and 'Item 1B. Unresolved Staff Comments'\n",
    "            pattern_1a = r'(?i)item\\s+1a\\.\\s+risk\\s+factors'\n",
    "            pattern_1b = r'(?i)item\\s+1b\\.\\s+unresolved\\s+staff\\s+comments'\n",
    "\n",
    "            matches_1a = re.finditer(pattern_1a, content)\n",
    "            positions_1a = [(match.start(),match.end()) for match in matches_1a]\n",
    "\n",
    "            matches_1b = re.finditer(pattern_1b, content)\n",
    "            positions_1b = [(match.start(),match.end()) for match in matches_1b]\n",
    "        \n",
    "            position, status = find_max_difference_pair(positions_1a,positions_1b)        \n",
    "            if (position is not None) and (status is None):\n",
    "                content = content [position[0]:position[1]]\n",
    "                content = capital_sentence_detect(content)\n",
    "                content = content.replace('[heading]','\\n\\n[heading]').replace('[/heading]','[/heading]\\n\\n')     \n",
    "                if len(re.findall(r'\\[heading\\].*?\\[/heading\\]', content)) >= 4:\n",
    "                    save_path_factor = os.path.join(save_folder_factor, os.path.splitext(file)[0] + '.txt')\n",
    "                    with open(save_path_factor, 'w+', encoding='utf-8') as f:\n",
    "                        f.write(content)\n",
    "                    f.close()\n",
    "                else:\n",
    "                    error_info = [cik, file]\n",
    "                    with open(error_txt_csv_path, 'a', newline='', encoding='utf-8') as error_file:\n",
    "                        error_writer = csv.writer(error_file)\n",
    "                        error_writer.writerow(error_info)\n",
    "            else:\n",
    "                error_info = [cik, file]\n",
    "                with open(error_txt_csv_path, 'a', newline='', encoding='utf-8') as error_file:\n",
    "                    error_writer = csv.writer(error_file)\n",
    "                    error_writer.writerow(error_info)\n",
    "                \n",
    "                \n",
    "        if os.path.splitext(read_path)[1] == '.html':\n",
    "            with open(read_path, 'r',encoding='utf-8') as f:\n",
    "                content = f.read()\n",
    "            f.close()\n",
    "            content = content.replace('&#160;', ' ').replace('&nbsp;', ' ')\n",
    "            pattern = r'Item 1A\\.(\\n|\\s)*Risk Factors<\\/[a-zA-Z]+>(<\\/[a-zA-Z]+>)* <[a-zA-Z]+>\\(Continued\\)|ITEM 1A\\.(.{0,10})RISK FACTORS (.{0,10})\\(continued\\)'\n",
    "            content = re.sub(pattern, '', content, flags=re.IGNORECASE)\n",
    "            soup = BeautifulSoup(content, 'html.parser')\n",
    "            soup = find_page_break_tags(soup)\n",
    "            soup = find_headings(soup)\n",
    "            text_list = [text for text in soup.stripped_strings]\n",
    "            if text_list[-1] != 'split_of_pages':\n",
    "                text_list.append('split_of_pages')\n",
    "            text_list = remove_page_footer(text_list)\n",
    "            text_list = [text for text in text_list if text != 'split_of_pages']\n",
    "            content = ' '.join(text_list)\n",
    "            content = combine_adjacent_headings(content)\n",
    "            \n",
    "            pattern = r'\\[heading\\](.*?)\\[/heading\\]'\n",
    "            content = re.sub(pattern, process_headings, content, flags=re.DOTALL)\n",
    "            \n",
    "            first_heading_position = find_heading_positions_with_keyword(content, ['item1a'])\n",
    "            second_heading_position = find_heading_positions_with_keyword(content, ['item1b','item2','item3'])\n",
    "            position,status  = find_max_difference_pair(first_heading_position, second_heading_position)\n",
    "            if (position is not None) and (status is None):\n",
    "                result = content[position[0]:position[1]]\n",
    "                result = re.sub(r'\\s+', ' ', result)\n",
    "                result = result.replace('[heading]','\\n[heading]').replace('[/heading]','[/heading]\\n')\n",
    "                result = remove_heading_risk_factors(result)\n",
    "                result = remove_heading_risk_factors_continued(result)\n",
    "                result = split_title_heading_content(result)\n",
    "                result = filter_content_between_heading_pairs(result)\n",
    "                result = result.replace('[title]','\\n[title]').replace('[/title]','[/title]\\n')\n",
    "                \n",
    "                if len(re.findall(r'\\[heading\\].*?\\[/heading\\]', result)) >= 5:  # Check if there are at least 10 headings\n",
    "                    save_path_factor = os.path.join(save_folder_factor, os.path.splitext(file)[0] + '.txt')\n",
    "                    with open(save_path_factor, 'w+', encoding='utf-8') as f:\n",
    "                        f.write(result)\n",
    "                    f.close()\n",
    "                else:\n",
    "                    first_heading_position = find_heading_positions_with_keyword(content, ['Risk Factors'])\n",
    "                    second_heading_position = find_heading_positions_with_keyword(content, ['Unresolved Staff Comments','Properties','Legal Proceedings'])\n",
    "                    position,status  = find_max_difference_pair(first_heading_position, second_heading_position)\n",
    "                    if (position is not None) and (status is None):\n",
    "                        result = content[position[0]:position[1]]\n",
    "                        result = re.sub(r'\\s+', ' ', result)\n",
    "                        result = result.replace('[heading]','\\n[heading]').replace('[/heading]','[/heading]\\n')\n",
    "                        result = remove_heading_risk_factors(result)\n",
    "                        result = remove_heading_risk_factors_continued(result)\n",
    "                        result = split_title_heading_content(result)\n",
    "                        result = filter_content_between_heading_pairs(result)\n",
    "                        result = result.replace('[title]','\\n[title]').replace('[/title]','[/title]\\n')\n",
    "                        if len(re.findall(r'\\[heading\\].*?\\[/heading\\]', result)) >= 5:  # Check if there are at least 10 headings\n",
    "                            save_path_factor = os.path.join(save_folder_factor, os.path.splitext(file)[0] + '.txt')\n",
    "                            with open(save_path_factor, 'w+', encoding='utf-8') as f:\n",
    "                                f.write(result)\n",
    "                            f.close()\n",
    "                        else:\n",
    "                            save_error_info(cik, file, error_html_csv_path)\n",
    "                    else:\n",
    "                        save_error_info(cik, file, error_html_csv_path)\n",
    "            else:\n",
    "                first_heading_position = find_heading_positions_with_keyword(content, ['Risk Factors'])\n",
    "                second_heading_position = find_heading_positions_with_keyword(content, ['Unresolved Staff Comments','Properties','Legal Proceedings'])\n",
    "                position,status  = find_max_difference_pair(first_heading_position, second_heading_position)\n",
    "                if (position is not None) and (status is None):\n",
    "                    result = content[position[0]:position[1]]\n",
    "                    result = re.sub(r'\\s+', ' ', result)\n",
    "                    result = result.replace('[heading]','\\n[heading]').replace('[/heading]','[/heading]\\n')\n",
    "                    result = remove_heading_risk_factors(result)\n",
    "                    result = remove_heading_risk_factors_continued(result)\n",
    "                    result = split_title_heading_content(result)\n",
    "                    result = filter_content_between_heading_pairs(result)\n",
    "                    result = result.replace('[title]','\\n[title]').replace('[/title]','[/title]\\n')\n",
    "                    if len(re.findall(r'\\[heading\\].*?\\[/heading\\]', result)) >= 5:  # Check if there are at least 10 headings\n",
    "                        save_path_factor = os.path.join(save_folder_factor, os.path.splitext(file)[0] + '.txt')\n",
    "                        with open(save_path_factor, 'w+', encoding='utf-8') as f:\n",
    "                            f.write(result)\n",
    "                        f.close()\n",
    "                    else:\n",
    "                        save_error_info(cik, file, error_html_csv_path)\n",
    "                else:\n",
    "                    save_error_info(cik, file, error_html_csv_path)\n",
    "                \n",
    "                    \n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "99b3a6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_files_for_cik_with_italic(cik):    \n",
    "    read_folder = os.path.join(root_folder, cik)\n",
    "    save_folder_factor = os.path.join(root_folder_risk_factors, cik)\n",
    "    if not os.path.exists(save_folder_factor):\n",
    "        os.makedirs(save_folder_factor)\n",
    "    if read_folder == 'data/.DS_Store':\n",
    "            return\n",
    "    for file in os.listdir(read_folder):\n",
    "        \n",
    "        read_path = os.path.join(read_folder, file)\n",
    "        \n",
    "        #txt format reports\n",
    "        if os.path.splitext(read_path)[1] == '.txt':\n",
    "            content = []\n",
    "            with open(read_path, 'r', encoding='utf-8') as input_txt:\n",
    "                for line in input_txt:\n",
    "                    row = line.strip().split()  # Assuming the file contains space-separated values\n",
    "                    if not contains_only_numbers(row): #filter page numbers\n",
    "                        content.append(line)\n",
    "            \n",
    "            #remove irrelevant tags\n",
    "            content = ''.join(content)\n",
    "            content = re.sub(r'\\s+', ' ', content)\n",
    "            content = re.sub(r'\\n.*\\n<PAGE>', '', content)\n",
    "            content = re.sub(r'\\n.*\\n</TEXT>', '', content)\n",
    "            content = re.sub(r'<[^>]+>', '', content)\n",
    "            \n",
    "            # Find positions of 'ITEM 1A. RISK FACTORS' and 'Item 1B. Unresolved Staff Comments'\n",
    "            pattern_1a = r'(?i)item\\s+1a\\.\\s+risk\\s+factors'\n",
    "            pattern_1b = r'(?i)item\\s+1b\\.\\s+unresolved\\s+staff\\s+comments'\n",
    "\n",
    "            matches_1a = re.finditer(pattern_1a, content)\n",
    "            positions_1a = [(match.start(),match.end()) for match in matches_1a]\n",
    "\n",
    "            matches_1b = re.finditer(pattern_1b, content)\n",
    "            positions_1b = [(match.start(),match.end()) for match in matches_1b]\n",
    "            \n",
    "            #Get the position pairs that are more likely to be correct\n",
    "            position, status = find_max_difference_pair(positions_1a,positions_1b)        \n",
    "            if (position is not None) and (status is None):\n",
    "                content = content [position[0]:position[1]]\n",
    "                content = capital_sentence_detect(content) #find headings\n",
    "                content = content.replace('[heading]','\\n\\n[heading]').replace('[/heading]','[/heading]\\n\\n')  #add \\n for better view\n",
    "                \n",
    "                if len(re.findall(r'\\[heading\\].*?\\[/heading\\]', content)) >= 4: #check number of headings\n",
    "                    save_path_factor = os.path.join(save_folder_factor, os.path.splitext(file)[0] + '.txt')\n",
    "                    with open(save_path_factor, 'w+', encoding='utf-8') as f:\n",
    "                        f.write(content)\n",
    "                    f.close()\n",
    "                else:\n",
    "                    #log error\n",
    "                    error_info = [cik, file]\n",
    "                    with open(error_txt_csv_path, 'a', newline='', encoding='utf-8') as error_file:\n",
    "                        error_writer = csv.writer(error_file)\n",
    "                        error_writer.writerow(error_info)\n",
    "            else:\n",
    "                #log error\n",
    "                error_info = [cik, file]\n",
    "                with open(error_txt_csv_path, 'a', newline='', encoding='utf-8') as error_file:\n",
    "                    error_writer = csv.writer(error_file)\n",
    "                    error_writer.writerow(error_info)\n",
    "                \n",
    "        \n",
    "        #html format reports\n",
    "        if os.path.splitext(read_path)[1] == '.html':\n",
    "            with open(read_path, 'r',encoding='utf-8') as f:\n",
    "                content = f.read()\n",
    "            f.close()\n",
    "            #remove irrelevant character\n",
    "            content = content.replace('&#160;', ' ').replace('&nbsp;', ' ')\n",
    "            #remove Continued tags\n",
    "            pattern = r'Item 1A\\.(\\n|\\s)*Risk Factors<\\/[a-zA-Z]+>(<\\/[a-zA-Z]+>)* <[a-zA-Z]+>\\(Continued\\)|ITEM 1A\\.(.{0,10})RISK FACTORS (.{0,10})\\(continued\\)'\n",
    "            content = re.sub(pattern, '', content, flags=re.IGNORECASE)\n",
    "            \n",
    "            soup = BeautifulSoup(content, 'html.parser') #parse html\n",
    "            \n",
    "            soup = find_page_break_tags(soup) #replace page break tags with 'split_of_pages'\n",
    "            \n",
    "            soup = find_headings_with_italic(soup) #split headings\n",
    "            \n",
    "            #remove page footers\n",
    "            text_list = [text for text in soup.stripped_strings]\n",
    "            if text_list[-1] != 'split_of_pages':\n",
    "                text_list.append('split_of_pages')\n",
    "            text_list = remove_page_footer(text_list)\n",
    "            text_list = [text for text in text_list if text != 'split_of_pages']\n",
    "            content = ' '.join(text_list)\n",
    "            \n",
    "            content = combine_adjacent_headings(content) #remove nested tags\n",
    "            \n",
    "            #remove headings that do not contain number or alphabet\n",
    "            pattern = r'\\[heading\\](.*?)\\[/heading\\]'\n",
    "            content = re.sub(pattern, process_headings, content, flags=re.DOTALL)\n",
    "            \n",
    "            content = replace_heading_with_comma(content) #remove heading followed by comma\n",
    "            content = filter_illegal_heading(content) #remove illegal heading\n",
    "            \n",
    "            #get the position of keywords\n",
    "            first_heading_position = find_heading_positions_with_keyword(content, ['item1a'])\n",
    "            second_heading_position = find_heading_positions_with_keyword(content, ['item1b','item2','item3','STATEMENTS OF INCOME ANALYSIS'])\n",
    "            position,status  = find_max_difference_pair(first_heading_position, second_heading_position)\n",
    "            \n",
    "            if (position is not None) and (status is None):\n",
    "                result = content[position[0]:position[1]] #get content by position\n",
    "                result = re.sub(r'\\s+', ' ', result) #remove nested \\s\n",
    "                result = result.replace('[heading]','\\n[heading]').replace('[/heading]','[/heading]\\n')\n",
    "                \n",
    "                result = remove_heading_risk_factors(result)\n",
    "                result = remove_heading_risk_factors_continued(result)\n",
    "                \n",
    "                result = split_title_heading_content(result)  #split title from headings\n",
    "                \n",
    "                result = filter_content_between_heading_pairs(result) #check the content between headings\n",
    "                result = result.replace('[title]','\\n[title]').replace('[/title]','[/title]\\n')\n",
    "                result = remove_short_heading(result)\n",
    "                \n",
    "                #for better view\n",
    "                result = re.sub(r'\\s+', ' ', result)\n",
    "                result = result.replace('[heading]','\\n\\n[heading]').replace('[/heading]','[/heading]\\n\\n')\n",
    "                result = result.replace('[title]','\\n\\n[title]')\n",
    "                \n",
    "                if len(re.findall(r'\\[heading\\].*?\\[/heading\\]', result)) >= 5:  # Check if there are at least 5 headings\n",
    "                    save_path_factor = os.path.join(save_folder_factor, os.path.splitext(file)[0] + '.txt')\n",
    "                    with open(save_path_factor, 'w+', encoding='utf-8') as f:\n",
    "                        f.write(result)\n",
    "                    f.close()\n",
    "                else:\n",
    "                    #similar, different keywords\n",
    "                    first_heading_position = find_heading_positions_with_keyword(content, ['Risk Factors'])\n",
    "                    second_heading_position = find_heading_positions_with_keyword(content, ['Unresolved Staff Comments','Properties','Legal Proceedings','STATEMENTS OF INCOME ANALYSIS'])\n",
    "                    position,status  = find_max_difference_pair(first_heading_position, second_heading_position)\n",
    "                    if (position is not None) and (status is None):\n",
    "                        result = content[position[0]:position[1]]\n",
    "                        result = re.sub(r'\\s+', ' ', result)\n",
    "                        result = result.replace('[heading]','\\n[heading]').replace('[/heading]','[/heading]\\n')\n",
    "                        result = remove_heading_risk_factors(result)\n",
    "                        result = remove_heading_risk_factors_continued(result)\n",
    "                        result = split_title_heading_content(result)\n",
    "                        result = filter_content_between_heading_pairs(result)\n",
    "                        result = result.replace('[title]','\\n[title]').replace('[/title]','[/title]\\n')\n",
    "                        result = remove_short_heading(result)\n",
    "                        result = re.sub(r'\\s+', ' ', result)\n",
    "                        result = result.replace('[heading]','\\n\\n[heading]').replace('[/heading]','[/heading]\\n\\n')\n",
    "                        result = result.replace('[title]','\\n\\n[title]')\n",
    "                        if len(re.findall(r'\\[heading\\].*?\\[/heading\\]', result)) >= 5:  # Check if there are at least 5 headings\n",
    "                            save_path_factor = os.path.join(save_folder_factor, os.path.splitext(file)[0] + '.txt')\n",
    "                            with open(save_path_factor, 'w+', encoding='utf-8') as f:\n",
    "                                f.write(result)\n",
    "                            f.close()\n",
    "                        else:\n",
    "                            save_error_info(cik, file, error_html_csv_path)\n",
    "                    else:\n",
    "                        save_error_info(cik, file, error_html_csv_path)\n",
    "            else:\n",
    "                #similar, different keywords\n",
    "                first_heading_position = find_heading_positions_with_keyword(content, ['Risk Factors'])\n",
    "                second_heading_position = find_heading_positions_with_keyword(content, ['Unresolved Staff Comments','Properties','Legal Proceedings','STATEMENTS OF INCOME ANALYSIS'])\n",
    "                position,status  = find_max_difference_pair(first_heading_position, second_heading_position)\n",
    "                if (position is not None) and (status is None):\n",
    "                    result = content[position[0]:position[1]]\n",
    "                    result = re.sub(r'\\s+', ' ', result)\n",
    "                    result = result.replace('[heading]','\\n[heading]').replace('[/heading]','[/heading]\\n')\n",
    "                    result = remove_heading_risk_factors(result)\n",
    "                    result = remove_heading_risk_factors_continued(result)\n",
    "                    result = split_title_heading_content(result)\n",
    "                    result = filter_content_between_heading_pairs(result)\n",
    "                    result = result.replace('[title]','\\n[title]').replace('[/title]','[/title]\\n')\n",
    "                    result = remove_short_heading(result)\n",
    "                    result = re.sub(r'\\s+', ' ', result)\n",
    "                    result = result.replace('[heading]','\\n\\n[heading]').replace('[/heading]','[/heading]\\n\\n')\n",
    "                    result = result.replace('[title]','\\n\\n[title]')\n",
    "                    if len(re.findall(r'\\[heading\\].*?\\[/heading\\]', result)) >= 5:  # Check if there are at least 10 headings\n",
    "                        save_path_factor = os.path.join(save_folder_factor, os.path.splitext(file)[0] + '.txt')\n",
    "                        with open(save_path_factor, 'w+', encoding='utf-8') as f:\n",
    "                            f.write(result)\n",
    "                        f.close()\n",
    "                    else:\n",
    "                        save_error_info(cik, file, error_html_csv_path)\n",
    "                else:\n",
    "                    save_error_info(cik, file, error_html_csv_path)\n",
    "                \n",
    "                    \n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fe576cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_folder = 'data'\n",
    "root_folder_risk_factors = 'risk_factors'\n",
    "error_html_csv_path = 'error_html_log.csv'\n",
    "error_txt_csv_path = 'error_txt_log.csv'\n",
    "if os.path.exists(error_html_csv_path):\n",
    "    os.remove(error_html_csv_path)\n",
    "if os.path.exists(error_txt_csv_path):\n",
    "    os.remove(error_txt_csv_path)\n",
    "\n",
    "#concurrent run, take several hours\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=os.cpu_count()) as executor:\n",
    "    futures = []\n",
    "    for cik in os.listdir(root_folder):\n",
    "        future = executor.submit(process_files_for_cik_with_italic, cik)\n",
    "        futures.append(future)\n",
    "    \n",
    "    # Wait for all tasks to complete\n",
    "    for future in futures:\n",
    "        future.result()\n",
    "    \n",
    "    # All tasks are completed, shutdown the executor\n",
    "    executor.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d16cce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
