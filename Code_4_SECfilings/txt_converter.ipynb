{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from collections import Counter\n",
    "import concurrent.futures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find page break tags and replace with keyword 'split_of_pages'\n",
    "def find_page_break_tags(soup):\n",
    "    #first format of page tag\n",
    "    page_break_tags = soup.find_all('hr', color=\"#999999\")\n",
    "    #second format of page tag\n",
    "    if len(page_break_tags) < 6:\n",
    "        page_break_tags = soup.find_all('hr')\n",
    "    #third format of page tag - find <div> elements with style 'page-break-after: always'\n",
    "    if len(page_break_tags) < 6:\n",
    "        page_break_tags = soup.find_all('div', style=lambda value: (value \n",
    "                                                            and re.search(r'page-break-after\\s*:\\s*always', value, re.IGNORECASE) \n",
    "                                                            and 'position:relative' not in value))                        \n",
    "    #replace page tags with split_of_pages\n",
    "    for tag in page_break_tags:\n",
    "        tag.replace_with('split_of_pages')\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find heading tags and add [heading][/heading]\n",
    "def find_headings_with_italic(soup):\n",
    "    #heading tag with html style bold or special color\n",
    "    for tag in (soup.find_all(style=lambda value: (value and (\n",
    "                'font-weight:bold' in value.lower() or\n",
    "                'font-weight: bold' in value.lower() or\n",
    "                'font-weight:700' in value.lower() or\n",
    "                'font-weight: 700' in value.lower() or \n",
    "                'color:#0068b5' in value.lower() or\n",
    "                'font-size:22pt' in value.lower()\n",
    "    )))):\n",
    "        content = tag.get_text()\n",
    "        tag.string = f\"\\n[heading]{content}[/heading]\\n\"\n",
    "                    \n",
    "                    \n",
    "    #heading tag b                   \n",
    "    for tag in soup.find_all('b'):\n",
    "        content = tag.get_text()\n",
    "        tag.string = f\"\\n[heading]{content}[/heading]\\n\"\n",
    "    \n",
    "    #heading tag strong   \n",
    "    for tag in (soup.find_all('strong')):\n",
    "        content = tag.get_text()\n",
    "        tag.string = f\"\\n[heading]{content}[/heading]\\n\"\n",
    "    \n",
    "    #heading tag em             \n",
    "    for tag in (soup.find_all('em')):\n",
    "        content = tag.get_text()\n",
    "        tag.string = f\"\\n[heading]{content}[/heading]\\n\"\n",
    "    \n",
    "    #heading tag with html style underline               \n",
    "    for tag in (soup.find_all(style=lambda value: (value and (\n",
    "        'text-decoration:underline' in value.lower() or\n",
    "        'text-decoration: underline' in value.lower())))):\n",
    "        content = tag.get_text()\n",
    "        tag.string = f\"\\n[heading]{content}[/heading]\\n\"\n",
    "    \n",
    "    #heading tag with html style italic \n",
    "    for tag in soup.find_all(True, style=True):\n",
    "        style_value = tag.get('style')  # Get the style attribute value\n",
    "        if style_value and ('italic' in style_value.lower()):\n",
    "            content = tag.get_text()\n",
    "            tag.string = f\"\\n[heading]{content}[/heading]\\n\" \n",
    "    \n",
    "    #heading tag i   \n",
    "    for tag in (soup.find_all('i')):\n",
    "        content = tag.get_text()\n",
    "        tag.string = f\"\\n[heading]{content}[/heading]\\n\"\n",
    "        \n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#judge whether the elements near 'split of page' is page footer or not\n",
    "def remove_page_footer_by_line(text_list,position,min_occurrence,footer_type='text'):\n",
    "    # get the elements to be deleted by label 'split_of_pages'\n",
    "    if footer_type == 'length':\n",
    "        #if forward\n",
    "        if position<0:\n",
    "            delete_elements = [len(text_list[i +position]) for i in range(-position,len(text_list)) if text_list[i] == 'split_of_pages']\n",
    "            # get the content of elements whose occurrences are greater than occurrence\n",
    "            counter = Counter(delete_elements)\n",
    "            length_to_delete = [length for length, count in counter.items() if count >= min_occurrence]\n",
    "            # Iterate in reverse order and delete the corresponding element\n",
    "            for i in range(len(text_list) - 1, -position-1, -1):\n",
    "                if text_list[i] == 'split_of_pages' and len(text_list[i +position]) in length_to_delete:\n",
    "                    if text_list[i +position]!='split_of_pages':\n",
    "                        del text_list[i +position]\n",
    "        #if backward\n",
    "        if position>0:\n",
    "            delete_elements = [len(text_list[i +position]) for i in range(len(text_list)-position) if text_list[i] == 'split_of_pages']\n",
    "            # get the content of elements whose occurrences are greater than occurrence\n",
    "            counter = Counter(delete_elements)\n",
    "            length_to_delete = [length for length, count in counter.items() if count >= min_occurrence]\n",
    "            # Iterate in reverse order and delete the corresponding element\n",
    "            for i in range(len(text_list) -position-1 , -1, -1):\n",
    "                if text_list[i] == 'split_of_pages' and len(text_list[i +position]) in length_to_delete:\n",
    "                    if text_list[i +position]!='split_of_pages':\n",
    "                        del text_list[i +position]\n",
    "                        \n",
    "                        \n",
    "    if footer_type == 'text':\n",
    "        #if forward\n",
    "        if position<0:\n",
    "            delete_elements = [text_list[i +position] for i in range(-position,len(text_list)) if text_list[i] == 'split_of_pages']\n",
    "            # get the content of elements whose occurrences are greater than occurrence\n",
    "            counter = Counter(delete_elements)\n",
    "            texts_to_delete = [text for text, count in counter.items() if count >= min_occurrence]\n",
    "\n",
    "            # Iterate in reverse order and delete the corresponding element\n",
    "            for i in range(len(text_list) - 1, -position-1, -1):\n",
    "                if text_list[i] == 'split_of_pages' and text_list[i +position] in texts_to_delete:\n",
    "                    if text_list[i +position]!='split_of_pages':\n",
    "                        del text_list[i +position]\n",
    "        #if backward\n",
    "        if position>0:\n",
    "            delete_elements = [text_list[i +position] for i in range(len(text_list)-position) if text_list[i] == 'split_of_pages']\n",
    "            # get the content of elements whose occurrences are greater than occurrence\n",
    "            counter = Counter(delete_elements)\n",
    "            texts_to_delete = [text for text, count in counter.items() if count >= min_occurrence]\n",
    "\n",
    "            # Iterate in reverse order and delete the corresponding element\n",
    "            for i in range(len(text_list) -position-1 , -1, -1):\n",
    "                if text_list[i] == 'split_of_pages' and text_list[i +position] in texts_to_delete:\n",
    "                    if text_list[i +position]!='split_of_pages':\n",
    "                        del text_list[i +position]\n",
    "    return text_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove page footers by parameters generated from common page footers\n",
    "def remove_page_footer(text_list):\n",
    "\n",
    "    # Remove page footer, the third element forward\n",
    "    text_list = remove_page_footer_by_line(text_list,-3,3,'text')\n",
    "\n",
    "    # Remove page footer, the second element forward\n",
    "    text_list = remove_page_footer_by_line(text_list,-2,3,'text')\n",
    "    \n",
    "    # Remove page number, the second element forward\n",
    "    text_list = remove_page_footer_by_line(text_list,-2,5,'length')\n",
    "    \n",
    "    # Remove page number, the first element forward\n",
    "    text_list = remove_page_footer_by_line(text_list,-1,5,'length')\n",
    "    \n",
    "    # Remove page footer, backward\n",
    "    for i in range (6,0,-1):\n",
    "        text_list = remove_page_footer_by_line(text_list,i,5,'text')\n",
    "    \n",
    "    return text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine nested heading tags e.g. [heading][heading] and [/heading][/heading]\n",
    "def combine_adjacent_headings(content):\n",
    "    while re.search(r'\\[heading\\]\\s*\\[heading\\]', content):\n",
    "        content = re.sub(r'\\[heading\\]\\s*\\[heading\\]', '[heading]', content)\n",
    "    while re.search(r'\\[/heading\\]\\s*\\[/heading\\]', content):\n",
    "        content = re.sub(r'\\[/heading\\]\\s*\\[/heading\\]', '[/heading]', content)\n",
    "    return content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check and remove tags for headings that are followed by ','\n",
    "def replace_heading_with_comma(content):\n",
    "    pattern_heading = r'\\[heading\\](.*?)\\[/heading\\]'\n",
    "    matches = re.finditer(pattern_heading, content)\n",
    "    modified_content = content\n",
    "    \n",
    "    for match in matches:\n",
    "        heading_content = match.group(1)\n",
    "        next_chars_start = match.end()  # Position immediately after [/heading]\n",
    "        next_chars_end = next_chars_start + 3\n",
    "        \n",
    "        if ',' in content[next_chars_start:next_chars_end]:\n",
    "            to_replace = match.group(0)  # Entire [heading][/heading] pattern\n",
    "            modified_content = modified_content.replace(to_replace, heading_content)\n",
    "            \n",
    "    return modified_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove headings that are not likely to be heading\n",
    "def filter_illegal_heading(content):\n",
    "    pattern_heading = r'\\[heading\\](.*?)\\[/heading\\]'\n",
    "    matches = re.finditer(pattern_heading, content)\n",
    "    modified_content = content\n",
    "    \n",
    "    for match in matches:\n",
    "        heading_content = match.group(1)\n",
    "        next_chars_start = match.end()  # Position immediately after [/heading]\n",
    "        \n",
    "        # Find the first character that's not a space or a newline\n",
    "        non_whitespace_char = re.search(r'[^\\s\\n]', content[next_chars_start:])\n",
    "        \n",
    "        if non_whitespace_char and non_whitespace_char.group(0) == '[':\n",
    "            # This is a nested [heading] tag, do nothing\n",
    "            pass\n",
    "        elif non_whitespace_char and non_whitespace_char.group(0).islower():\n",
    "            # Replace the illegal [heading][/heading] pattern with just the heading content\n",
    "            to_replace = match.group(0)\n",
    "            modified_content = modified_content.replace(to_replace, heading_content, 1)\n",
    "            \n",
    "    return modified_content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove headings that do not contain number or character\n",
    "def process_headings(match):\n",
    "    content = match.group(1)\n",
    "    if any(c.isalnum() for c in content):\n",
    "        return '[heading]'+content+'[/heading]'\n",
    "    else:\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_fillings_for_cik(cik):\n",
    "        read_folder = os.path.join(root_folder, cik)\n",
    "        save_folder = os.path.join(root_folder_fillings, cik)\n",
    "        if not os.path.exists(save_folder):\n",
    "            os.makedirs(save_folder)\n",
    "        if read_folder == f'{root_folder}/.DS_Store':\n",
    "            return\n",
    "        for file in os.listdir(read_folder):\n",
    "            read_path = os.path.join(read_folder, file)\n",
    "            \n",
    "            save_path_filling = os.path.join(save_folder, os.path.splitext(file)[0] + '.txt')\n",
    "\n",
    "            if os.path.exists(save_path_filling):\n",
    "                continue\n",
    "            if os.path.splitext(read_path)[1] == '.html':\n",
    "                with open(read_path, 'r', encoding='utf-8') as f:\n",
    "                    content = f.read()\n",
    "                f.close()\n",
    "                \n",
    "                #remove irrelevant character\n",
    "                content = content.replace('&#160;', ' ').replace('&nbsp;', ' ')\n",
    "                #remove Continued tags\n",
    "                pattern = r'Item 1A\\.(\\n|\\s)*Risk Factors<\\/[a-zA-Z]+>(<\\/[a-zA-Z]+>)* <[a-zA-Z]+>\\(Continued\\)|ITEM 1A\\.(.{0,10})RISK FACTORS (.{0,10})\\(continued\\)'\n",
    "                content = re.sub(pattern, '', content, flags=re.IGNORECASE)\n",
    "                \n",
    "                soup = BeautifulSoup(content, 'html.parser') #parse html\n",
    "                \n",
    "                soup = find_page_break_tags(soup) #replace page break tags with 'split_of_pages'\n",
    "                \n",
    "                soup = find_headings_with_italic(soup) #split headings\n",
    "                \n",
    "                #remove page footers\n",
    "                text_list = [text for text in soup.stripped_strings]\n",
    "                if text_list[-1] != 'split_of_pages':\n",
    "                    text_list.append('split_of_pages')\n",
    "                text_list = remove_page_footer(text_list)\n",
    "                text_list = [text for text in text_list if text != 'split_of_pages']\n",
    "                content = ' '.join(text_list)\n",
    "                \n",
    "                content = combine_adjacent_headings(content) #remove nested tags\n",
    "                \n",
    "                #remove headings that do not contain number or alphabet\n",
    "                pattern = r'\\[heading\\](.*?)\\[/heading\\]'\n",
    "                content = re.sub(pattern, process_headings, content, flags=re.DOTALL)\n",
    "                \n",
    "                content = replace_heading_with_comma(content) #remove heading followed by comma\n",
    "                content = filter_illegal_heading(content) #remove illegal heading\n",
    "                \n",
    "                save_path_filling = os.path.join(save_folder, os.path.splitext(file)[0] + '.txt')\n",
    "\n",
    "                with open(save_path_filling, 'w', encoding='utf-8') as f:\n",
    "                    f.write(content)\n",
    "                f.close()\n",
    "                \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_folder = 'total_sp500_10k-html' # Input folder\n",
    "root_folder_fillings = 'total_sp500_10k-txt' # Output folder\n",
    "\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=os.cpu_count()) as executor:\n",
    "    futures = []\n",
    "    for cik in os.listdir(root_folder):\n",
    "\n",
    "        future = executor.submit(process_fillings_for_cik, cik)\n",
    "        futures.append(future)\n",
    "        \n",
    "        \n",
    "    # Wait for all tasks to complete\n",
    "    for future in futures:\n",
    "        future.result()\n",
    "    \n",
    "    # All tasks are completed, shutdown the executor\n",
    "    executor.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_folder = 'total_sp500_10q-html'\n",
    "root_folder_fillings = 'total_sp500_10q-txt'\n",
    "\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=os.cpu_count()) as executor:\n",
    "    futures = []\n",
    "    for cik in os.listdir(root_folder):\n",
    "\n",
    "        future = executor.submit(process_fillings_for_cik, cik)\n",
    "        futures.append(future)\n",
    "        \n",
    "        \n",
    "    # Wait for all tasks to complete\n",
    "    for future in futures:\n",
    "        future.result()\n",
    "    \n",
    "    # All tasks are completed, shutdown the executor\n",
    "    executor.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hons-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
