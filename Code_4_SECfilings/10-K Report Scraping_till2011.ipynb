{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c5de3acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "from ratelimit import limits, sleep_and_retry\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from collections import Counter\n",
    "import re\n",
    "import csv\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d58d8e7",
   "metadata": {},
   "source": [
    "Get CIKs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "222796e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "QQQ_path = './update_and_only2025.csv'\n",
    "\n",
    "# QQQ_path = './test.csv'\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(QQQ_path, encoding = 'utf-8')\n",
    "    QQQ_cik = df['CIK'].drop_duplicates().tolist()\n",
    "    QQQ_ticker = df['Symbol'].tolist()\n",
    "    QQQ_cik_ticker = dict(zip(QQQ_cik, QQQ_ticker))\n",
    "except UnicodeDecodeError:\n",
    "    df = pd.read_csv(QQQ_path, encoding = 'ISO-8859-1')\n",
    "    QQQ_cik = df['CIK'].drop_duplicates().tolist()\n",
    "    QQQ_ticker = df['Symbol'].tolist()\n",
    "    QQQ_cik_ticker = dict(zip(QQQ_cik, QQQ_ticker))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2754987e",
   "metadata": {},
   "source": [
    "Download Reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7d358486",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LimitRequest(object):\n",
    "    SEC_CALL_LIMIT = {'calls': 10, 'seconds': 1}\n",
    "    @sleep_and_retry\n",
    "    @limits(calls=SEC_CALL_LIMIT['calls'], period=SEC_CALL_LIMIT['seconds'])\n",
    "    def _call_sec(url,headers):\n",
    "        return requests.get(url,headers=headers)\n",
    "    \n",
    "    @classmethod\n",
    "    def get(cls,url,headers):\n",
    "        return cls._call_sec(url, headers)\n",
    "\n",
    "\n",
    "def get_sec_data(cik, doc_type, headers,end_date, start_date, start, count):\n",
    "    start_date = pd.to_datetime(start_date)\n",
    "    end_date = pd.to_datetime(end_date)\n",
    "    rss_url = 'https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany' \\\n",
    "        '&CIK={}&type={}&start={}&count={}&owner=exclude&output=atom' \\\n",
    "        .format(cik, doc_type, start, count)\n",
    "    \n",
    "    sec_data = LimitRequest.get(url = rss_url,headers=headers)\n",
    "    soup = BeautifulSoup(sec_data.content, 'xml')    \n",
    "    entries = [\n",
    "        (   entry.content.find('filing-href').getText(),\n",
    "            entry.content.find('filing-type').getText(),\n",
    "            entry.content.find('filing-date').getText())\n",
    "        for entry in soup.find_all('entry')\n",
    "        if pd.to_datetime(entry.content.find('filing-date').getText()) <= end_date and pd.to_datetime(entry.content.find('filing-date').getText()) >= start_date]  \n",
    "    return entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9dbda4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_document_type(doc):\n",
    "    \"\"\"\n",
    "    Return the document type lowercased\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    doc : str\n",
    "        The document string\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    doc_type : str\n",
    "        The document type lowercased\n",
    "    \"\"\"\n",
    "    \n",
    "    # Regex explaination : Here I am tryng to do a positive lookbehind\n",
    "    # (?<=a)b (positive lookbehind) matches the b (and only the b) in cab, but does not match bed or debt.\n",
    "    # More reference : https://www.regular-expressions.info/lookaround.html\n",
    "    \n",
    "    type_regex = re.compile(r'(?<=<TYPE>)\\w+[^\\n]+') # gives out \\w\n",
    "    type_idx = re.search(type_regex, doc).group(0).lower()\n",
    "    return type_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2b481e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_document_format(doc):\n",
    "    \"\"\"\n",
    "    Return the document type lowercased\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    doc : str\n",
    "        The document string\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    doc_type : str\n",
    "        The document type lowercased\n",
    "    \"\"\"\n",
    "    \n",
    "    format_regex = re.compile(r'(?<=<FILENAME>)\\w+[^\\n]+') # gives out \\w\n",
    "    doc_type  = re.search(format_regex, doc).group(0).lower()\n",
    "    if doc_type.endswith((\".htm\", \".html\")):\n",
    "        return 'HTML'\n",
    "    if doc_type.endswith(\".txt\"):\n",
    "        return 'TXT'\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7419ffca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_documents(text):\n",
    "    document_start_regex = re.compile(r'<DOCUMENT>')\n",
    "    document_end_regex = re.compile(r'<\\/DOCUMENT>')\n",
    "    \n",
    "    document_start_indices = [match.start() for match in document_start_regex.finditer(text)]\n",
    "    document_end_indices = [match.start() for match in document_end_regex.finditer(text)]\n",
    "    \n",
    "    documents = []\n",
    "    for start_index, end_index in zip(document_start_indices, document_end_indices):\n",
    "        document = text[start_index:end_index]\n",
    "        documents.append(document)\n",
    "    \n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0f9fdd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def download_fillings(ciks, root_folder, doc_type, headers, end_date=datetime.datetime.now(), start_date = '1990-01-01', start=0, count=60):\n",
    "    doc_type= doc_type.lower()\n",
    "    for cik in ciks:\n",
    "        cik = str(cik).zfill(10)\n",
    "        report_info = get_sec_data(cik, doc_type, headers, end_date=end_date, start_date=start_date, start=start, count=count)\n",
    "        # check if 10-K exists, otherwise skip it\n",
    "        if not report_info:\n",
    "            continue\n",
    "        else:\n",
    "            folder_path = os.path.join(root_folder, cik)\n",
    "            if not os.path.exists(folder_path):\n",
    "                os.makedirs(folder_path)\n",
    "\n",
    "        for index_url, file_type, file_date in tqdm(report_info, desc='Downloading {} Fillings'.format(cik), unit='filling'):\n",
    "            if (file_type.lower() == doc_type):\n",
    "                file_url = index_url.replace('-index.htm', '.txt').replace('.txtl', '.txt')\n",
    "                file = LimitRequest.get(url=file_url, headers=headers)\n",
    "                for document in get_documents(file.text):\n",
    "                    if get_document_type(document) == doc_type and get_document_format(document) == 'HTML':\n",
    "                        file_name = os.path.join(folder_path, file_date + '.html')\n",
    "                        with open(file_name,'w+') as f:\n",
    "                            f.write(document)\n",
    "                        f.close()\n",
    "                    if get_document_type(document) == doc_type and get_document_format(document) == 'TXT':\n",
    "                        file_name = os.path.join(folder_path, file_date + '.txt')\n",
    "                        with open(file_name,'w+') as f:\n",
    "                            f.write(document)\n",
    "                        f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4acdbf05",
   "metadata": {},
   "source": [
    "Report starts from 2006; parts of 2005 reports don't have item1A <br>\n",
    "Only for 10-k reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ecc75972",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading 0001067983 Fillings: 100%|██████████| 1/1 [00:00<00:00,  1.89filling/s]\n",
      "Downloading 0001404912 Fillings: 100%|██████████| 1/1 [00:00<00:00,  1.99filling/s]\n",
      "Downloading 0001375365 Fillings: 100%|██████████| 2/2 [00:00<00:00,  2.36filling/s]\n",
      "Downloading 0001175454 Fillings: 100%|██████████| 1/1 [00:00<00:00,  2.58filling/s]\n",
      "Downloading 0001069202 Fillings: 100%|██████████| 1/1 [00:00<00:00,  2.14filling/s]\n",
      "Downloading 0000765880 Fillings: 100%|██████████| 1/1 [00:00<00:00,  1.84filling/s]\n",
      "Downloading 0000922621 Fillings: 100%|██████████| 1/1 [00:00<00:00,  1.56filling/s]\n",
      "Downloading 0000014693 Fillings: 100%|██████████| 1/1 [00:00<00:00,  1.76filling/s]\n",
      "Downloading 0000798354 Fillings: 100%|██████████| 1/1 [00:00<00:00,  1.89filling/s]\n",
      "Downloading 0000216228 Fillings: 100%|██████████| 1/1 [00:01<00:00,  1.14s/filling]\n",
      "Downloading 0000064040 Fillings: 100%|██████████| 1/1 [00:00<00:00,  2.21filling/s]\n",
      "Downloading 0000101829 Fillings: 100%|██████████| 1/1 [00:00<00:00,  1.82filling/s]\n",
      "Downloading 0001109357 Fillings: 100%|██████████| 1/1 [00:01<00:00,  1.52s/filling]\n",
      "Downloading 0001130310 Fillings: 100%|██████████| 1/1 [00:00<00:00,  1.46filling/s]\n",
      "Downloading 0000072903 Fillings: 100%|██████████| 1/1 [00:00<00:00,  1.14filling/s]\n",
      "Downloading 0000753308 Fillings: 100%|██████████| 1/1 [00:00<00:00,  1.94filling/s]\n",
      "Downloading 0000004281 Fillings: 100%|██████████| 1/1 [00:00<00:00,  2.73filling/s]\n",
      "Downloading 0000060086 Fillings: 100%|██████████| 1/1 [00:00<00:00,  2.33filling/s]\n",
      "Downloading 0000047217 Fillings: 100%|██████████| 1/1 [00:00<00:00,  1.40filling/s]\n",
      "Downloading 0000004447 Fillings: 100%|██████████| 1/1 [00:00<00:00,  1.88filling/s]\n",
      "Downloading 0001281761 Fillings: 100%|██████████| 1/1 [00:00<00:00,  1.11filling/s]\n",
      "Downloading 0000036966 Fillings: 100%|██████████| 1/1 [00:00<00:00,  1.90filling/s]\n",
      "Downloading 0000026172 Fillings: 100%|██████████| 1/1 [00:00<00:00,  2.39filling/s]\n",
      "Downloading 0000031791 Fillings: 100%|██████████| 1/1 [00:00<00:00,  1.82filling/s]\n",
      "Downloading 0001059556 Fillings: 100%|██████████| 1/1 [00:00<00:00,  2.64filling/s]\n",
      "Downloading 0000009389 Fillings: 100%|██████████| 1/1 [00:00<00:00,  2.49filling/s]\n",
      "Downloading 0000024545 Fillings: 100%|██████████| 1/1 [00:00<00:00,  2.15filling/s]\n",
      "Downloading 0000086312 Fillings: 100%|██████████| 1/1 [00:00<00:00,  2.01filling/s]\n",
      "Downloading 0000018926 Fillings: 100%|██████████| 1/1 [00:00<00:00,  2.73filling/s]\n",
      "Downloading 0000315293 Fillings: 100%|██████████| 1/1 [00:00<00:00,  2.32filling/s]\n",
      "Downloading 0000701985 Fillings: 100%|██████████| 1/1 [00:00<00:00,  1.29filling/s]\n",
      "Downloading 0000732712 Fillings: 100%|██████████| 1/1 [00:00<00:00,  2.63filling/s]\n",
      "Downloading 0000895421 Fillings: 100%|██████████| 1/1 [00:00<00:00,  1.92filling/s]\n",
      "Downloading 0000793952 Fillings: 100%|██████████| 1/1 [00:00<00:00,  1.18filling/s]\n",
      "Downloading 0000316709 Fillings: 100%|██████████| 1/1 [00:00<00:00,  2.54filling/s]\n",
      "Downloading 0000813828 Fillings: 100%|██████████| 1/1 [00:00<00:00,  1.85filling/s]\n",
      "Downloading 0001466258 Fillings: 100%|██████████| 1/1 [00:00<00:00,  2.13filling/s]\n",
      "Downloading 0001136869 Fillings: 100%|██████████| 1/1 [00:00<00:00,  1.76filling/s]\n",
      "Downloading 0001138118 Fillings: 100%|██████████| 1/1 [00:00<00:00,  1.88filling/s]\n",
      "Downloading 0000096223 Fillings: 100%|██████████| 1/1 [00:00<00:00,  1.74filling/s]\n",
      "Downloading 0000104889 Fillings: 100%|██████████| 1/1 [00:00<00:00,  1.48filling/s]\n",
      "Downloading 0001103982 Fillings: 100%|██████████| 1/1 [00:00<00:00,  2.59filling/s]\n",
      "Downloading 0000202058 Fillings: 100%|██████████| 1/1 [00:00<00:00,  3.07filling/s]\n",
      "Downloading 0000858470 Fillings: 100%|██████████| 1/1 [00:00<00:00,  2.68filling/s]\n",
      "Downloading 0001075531 Fillings: 100%|██████████| 1/1 [00:00<00:00,  1.47filling/s]\n",
      "Downloading 0001140536 Fillings: 100%|██████████| 1/1 [00:00<00:00,  2.27filling/s]\n",
      "Downloading 0001095073 Fillings: 100%|██████████| 1/1 [00:00<00:00,  2.09filling/s]\n",
      "Downloading 0000048898 Fillings: 100%|██████████| 1/1 [00:00<00:00,  1.90filling/s]\n",
      "Downloading 0001069183 Fillings: 100%|██████████| 1/1 [00:00<00:00,  2.53filling/s]\n",
      "Downloading 0001316835 Fillings: 100%|██████████| 1/1 [00:00<00:00,  3.32filling/s]\n",
      "Downloading 0001393818 Fillings: 100%|██████████| 1/1 [00:00<00:00,  1.08filling/s]\n"
     ]
    }
   ],
   "source": [
    "root_folder = 'total_sp500_10q-html'\n",
    "doc_type = '10-Q'\n",
    "headers = {'User-Agent': 'University of Edinburgh s2101367@ed.ac.uk'}\n",
    "start_date = '2006-01-01',\n",
    "end_date = '2012-01-01'\n",
    "if not os.path.exists(root_folder):\n",
    "    os.makedirs(root_folder)\n",
    "download_fillings(QQQ_cik_ticker, root_folder,doc_type,headers,end_date=end_date,start_date=start_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b45047b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18386, 12387)\n",
      "(7539, 13677)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "path_10q = \"/Users/apple/PROJECT/hons_project/data/SP500/10Q/dtm/part-00000-c037b728-f21e-443d-8148-ae09abea42f7-c000.snappy.parquet\"\n",
    "df = pd.read_parquet(path_10q)\n",
    "print(df.shape)\n",
    "\n",
    "path_10k = \"/Users/apple/PROJECT/hons_project/data/SP500/10K/dtm/part-00000-bbf3389a-a3ed-4008-babf-f89139cffa93-c000.snappy.parquet\"\n",
    "df_10k = pd.read_parquet(path_10k)\n",
    "print(df_10k.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c97adab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hons-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
