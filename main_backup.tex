% UG project example file, February 2022
%   A minior change in citation, September 2023 [HS]
% Do not change the first two lines of code, except you may delete "logo," if causing problems.
% Understand any problems and seek approval before assuming it's ok to remove ugcheck.
\documentclass[logo,bsc,singlespacing,parskip]{infthesis}
\usepackage{ugcheck}

% Include any packages you need below, but don't include any that change the page
% layout or style of the dissertation. By including the ugcheck package above,
% you should catch most accidental changes of page layout though.

\usepackage{microtype} % recommended, but you can remove if it causes problems
\usepackage{natbib} % recommended for citations
\usepackage{amsmath}

% table 
\documentclass{article}
\usepackage{multirow} % for multirow feature
\usepackage{array}    % for extended column definitions
\usepackage{tabularx} % for adjustable-width columns
\usepackage{graphicx}
\usepackage{titlesec}
\usepackage{hyperref}
\usepackage{subcaption}
\usepackage[normalem]{ulem}
\usepackage{caption}

\usepackage{nonfloat}
% \makeatletter
% \setlength{\@fptop}{0pt}
% \setlength{\@fpbottom}{0pt}
% \makeatother

% \hypersetup{
%     colorlinks= true,
%     linkcolor=blue,
%     filecolor=magenta,      
%     urlcolor=cyan,
%     pdftitle={Overleaf Example},
%     pdfpagemode=FullScreen,
%     linkbordercolor = {1, 0, 0},
%     }
\urlstyle{same}


\raggedbottom

\begin{document}
\begin{preliminary}

\title{Generation of Sentiment Metrics for Return and Volatility Prediction from 10-K Fillings}

\author{Sanggyu Sean Choi}

% CHOOSE YOUR DEGREE a):
% please leave just one of the following un-commented
% \course{Artificial Intelligence}
%\course{Artificial Intelligence and Computer Science}
%\course{Artificial Intelligence and Mathematics}
%\course{Artificial Intelligence and Software Engineering}
%\course{Cognitive Science}
%\course{Computer Science}
%\course{Computer Science and Management Science}
%\course{Computer Science and Mathematics}
%\course{Computer Science and Physics}
%\course{Software Engineering}
\course{Master of Informatics} % MInf students

% CHOOSE YOUR DEGREE b):
% please leave just one of the following un-commented
\project{MInf Project (Part 1) Report}  % 4th year MInf students
%\project{MInf Project (Part 2) Report}  % 5th year MInf students
% \project{4th Year Project Report}        % all other UG4 students


\date{\today}

\abstract{
This skeleton demonstrates how to use the \texttt{infthesis} style for
undergraduate dissertations in the School of Informatics. It also emphasises the
page limit, and that you must not deviate from the required style.
The file \texttt{skeleton.tex} generates this document and should be used as a
starting point for your thesis. Replace this abstract text with a concise
summary of your report.

\textbf{Keywords}
Sentiment Analysis, Fundamental Analysis, Data Orchestration, Machine Learning, Return, Volatility, 10-K Fillings
}



\maketitle

\newenvironment{ethics}
   {\begin{frontenv}{Research Ethics Approval}{\LARGE}}
   {\end{frontenv}\newpage}

\begin{ethics}
\textbf{Instructions:} \emph{Agree with your supervisor which
statement you need to include. Then delete the statement that you are not using,
and the instructions in italics.\\
\textbf{Either complete and include this statement:}}\\ % DELETE THESE INSTRUCTIONS
%
% IF ETHICS APPROVAL WAS REQUIRED:
This project obtained approval from the Informatics Research Ethics committee.\\
Ethics application number: ???\\
Date when approval was obtained: YYYY-MM-DD\\
%
\emph{[If the project required human participants, edit as appropriate, otherwise delete:]}\\ % DELETE THIS LINE
The participants' information sheet and a consent form are included in the appendix.\\
%
% IF ETHICS APPROVAL WAS NOT REQUIRED:
\textbf{\emph{Or include this statement:}}\\ % DELETE THIS LINE
This project was planned in accordance with the Informatics Research
Ethics policy. It did not involve any aspects that required approval
from the Informatics Research Ethics committee.

\standarddeclaration
\end{ethics}


\begin{acknowledgements}
Any acknowledgements go here.
\end{acknowledgements}



\tableofcontents
\end{preliminary}


\chapter{Introduction}
\section{Motivation}

Now is the area of Big Data. With the advance of computational powers, a large amount of various data, such as text, video, and audio have been used for scientific analysis. Among a myriad of data forms, textual data has gotten the fastest attention in the social science academic field. Textual data’s numerical representation for statistical analysis in nature is extremely high in dimensions that empirical study seeking its textual richness should face its dimensionality challenges. Machine learning will be employed to extract richer meaning from textual data for predictive analysis in a high-dimensional data environment \cite{ke2020predicting}. 

In finance, textual data is commonly employed for predicting market movement. Then, what type of textual data can be informative for such a purpose? If you want to invest in a public company in the United States, where can you begin your investment journey? There are many ways to begin your investment, but you can find a rich deposit of knowledge in the company’s annual report on Form 10-K. Among other things, the 10-K is a comprehensive official document offering a thorough overview of a company’s business, its potential challenges, and its financial performance through the fiscal year. Moreover, in the 10-K, the company’s leadership provides their perspective on the business outcomes and the factors influencing them \cite{secinvestor2011}. 

Since the beginning of 2005, a new section has been required to be included in all firms’ annual fillings by the Securities and Exchange Commission (SEC). The section called “Section 1A of the Annual Report on Form 10-K” which discusses “the most significant factors that make the company speculative or risky” \cite{sec2005}. Prior to this alteration, companies were only obligated to provide this information in their registration when issuing their equity or debt securities. Also, some companies voluntarily offer risk disclosures in the section called “Management’s Discussion and Analysis of Financial Condition and Results of Operations(MD\&A)” 

Opponents of the new disclosure requirements argue that risk factor disclosures are unlikely to offer valuable information. First, risk disclosure can be biased. Managers might resist disclosing negative information about their business or career incentives 
\cite{wattszimmerman1986}\cite{scott1994}\cite{fieldsetal2001}\cite{kotharietal2009a}\cite{kotharietal2009b}. Second, managers’ overconfidence can make them perceive less risk or overconfident managers can have the illusion that they can effectively manage the risks confronting their firms \cite{chang2019}. Third, managers tend to disclose all possible risks and uncertainties without making precise predictions or providing detailed financial assessments. This practice stems from the fact that companies are not required to predict the possibility that a disclosed risk will actually materialise. Furthermore, there is no obligation for firms to specify the financial influence that a disclosed risk may have on their present or future financial statements \cite{reuters2005}. Since 2010, the SEC has warned companies to “avoid generic risk factor disclosure that could apply to any company” \cite{sec2010}, and has continuously pushed the precise risk factor disclosures through the comment letter process \cite{cfo2010}. Recently, the SEC has been demanding the explicit and specific inclusion of both cyber security risk factor disclosure and climate change risk factor disclosure \citep{kingsleyetal2021}\citep{peirce2023}. Including the continuous adoption of the SEC’s risk factor disclosure rules, we found evidence to support whether these newly established disclosures are informative.

Numerous studies have concluded that the risk factor disclosures provide valuable information. \cite{campbelletal2014a} found that the disclosures may help investors assess the volatility of a firm’s cash flows, and his further study said that tax-related risk factor disclosures offer details about the level of a firm’s future cash flows, helping investors incorporate this information into current stock prices. \cite{israelsen2014} also found that the newly created risk factor disclosures show a correlation with conventional asset pricing risk factors, indicating that the disclosed factors are valuable for assessing overall risk. Moreover, \cite{campbelletal2014a} found that lengthier risk factor disclosures can cause negative market reactions. \cite{hopeetal2016} argued more detailed disclosures tend to generate more profound market reactions. \cite{kravetmuslu2013} found that alterations in the length of risk disclosures can influence an investor’s risk perceptions. \cite{songetal2020} found that cyber security risk factor disclosures increase the risk of a company’s stock price declining in the future. 

Other research has explored aspects beyond the informative risk factor disclosures \cite{dyeretal2016}. \cite{beattychengzhang2015} found that the number of annual risk factor disclosures has been on the rise over the years. \cite{beattychengzhang2015} found that the lengthy disclosures may be decreasing informativeness over time, especially for the financial constraints part of the disclosures. \cite{baileyfilzen2016} found that the competent individual responsible for risk management within a company can manage well the level of risk factors disclosure. When a firm has a more experienced Chief Risk Officer, the market tends to have a less strong reaction to lengthier disclosures. \cite{filzenmcbrayershannon2016} found that risk factor *updates* can reduce future returns while explicit update contents exhibit the highest predictability for future returns.

\section{Project Objective}
With the informative 10-K fillings mentioned in the motivation section, our project aims to achieve the following main goal:

\begin{itemize}
    \item Developing an automated pipeline to generate sentiment scores from 10-K reports of firms in the technology industry for market movement prediction of a firm, i.e., return and volatility.
\end{itemize}

The following are the sub-steps for the main goal:

\begin{itemize}
    \item Extraction of risk factors from 10-K reports listed on the Invesco QQQ Trust Series 1 (QQQ).
\end{itemize}

Our pipeline will collect an annual disclosure from the SEC’s Electronic Data Gathering, Analysis, and Retrieval(EDGAR) system, and extract only the risk factor of each report. For a meaningful study, this extraction model will be designed to collect only firms in the technology industry listed on the QQQ. In this research, our pipeline initially collected the top 10 technology firms, and all technology companies listed on the QQQ. The top 10 firms on the QQQ account for 50\% of the total portfolio of the QQQ. However, our pipeline is scalable. It can easily be expanded to extract any other fields as well.

\begin{itemize}
    \item Generate various sentiment scores from the extracted 10-K reports.
\end{itemize}

In this research, our study aims to generate sentiment metrics for market movement prediction of a firm, i.e., return and volatility, from informative risk factor disclosures. This is the main part of my project. Recently, a new model has been proposed to generate strong indicators for predicting price reactions to new information. This model \cite{ke2020predicting} generates a powerful return-predictive signal from their supervised sentiment text model with news data. Our research methodology for generating sentiment scores refers to the methods suggested in \cite{ke2020predicting}. However, our study showed an innovative improvement compared to \cite{ke2020predicting}.  \cite{ke2020predicting} employed news articles only to generate return-predictive signals, while our study used 10-K reports, especially focusing on the risk factor section, to generate volatility-predictive signals as well as return-predictive signals. As far as we are aware, no research applies supervised sentiment learning with 10-K reports for predicting volatility, nor is there any that offers a comparative study of pre-established and acquired sentiments by using 10-K reports for returns or volatility predictions.

\begin{itemize}
    \item Build an automated pipeline on the Airflow framework.
\end{itemize}

\begin{itemize}
    \item Evaluate sentiment scores in the context of prediction for returns and volatility.
\end{itemize}


\section{Contribution}
\section{Outline of the Project}



% Citations, such as \citet{P1} or \citep{P2}, can be generated using
% \texttt{BibTeX}. We recommend using the \texttt{natbib} package (default) or the newer \texttt{biblatex} system. 

% You may use any consistent reference style that you prefer, including ``(Author, Year)'' citations. 

\chapter{Related Works}
In order to make our study aim work, our study will employ textual analysis from informative risk factors in 10-K fillings. In contemporary literature, \citep{wang2016financial} examined the occurrence of risk-related terms in the 10-K reports before the newly established risk factors are disclosed. The findings indicate that future earning declines when the number of risk sentiment rises. Furthermore, the study find that investors tend to underreact to the disclosures. [18] examined the occurrence of risk sentences in the all 10-K reports across years, both before and after the introduction of the newly created risk factor section. The finding shows a connection between the alteration in the count of risk sentences and how investors assess a company’s idiosyncratic risk, which relates to stock return volatility. Additionally, the changes are associated with the properties of analyst earnings forecast. [14] study complemented the work of \citep{wang2016financial} and \citep{kravetmuslu2013}. In \citep{campbelletal2014a}, they firstly shows an evidence that investors exhibits a partial response to risk disclosures. Second, \citep{campbelletal2014a} focuses strictly on the newly created risk factor section, confirming the result of \citep{kravetmuslu2013} regarding investors’ assessment of idiosyncratic risk. Finally, they employ a word counting and classification tactic to support that the risk disclosures offered by managers are not mere boilerplate, but instead reflect the genuine risks confronting their firms. 

A dissertation usually contains several chapters.
\section{Econometric approaches}
hello AI world
\section{AI approaches}
\section{Supervised lexicon learning}
Ke et al.(2019) have proposed a relatively transparent and simple supervised lexicon learning model to find the correlation between firm-specific news and returns. Their hypothesis of the research is that the number of sentiment-charged words in news articles relies on **return-relevant unobserved sentiment scores**. Their model suggests that these word counts are derived from **a mixture multinomial distributio**n, in which the sentiment score controls the equilibrium between distributions of words with maximally positive and negative connotations. A detail explanation of the model will be found in Section 4. The model then conducted a comparison between their developed work dictionary to the widely recognised Harvard-IV( Stone et al, 1966) and LM (Loughran and McDonald, 2011) word lists, uncovering notable differences. 

As far as we aware, there exists no existing research that applies supervised lexicon learning specifically for volatility prediction with 10-K reports, nor any that compares, with 10-K reports, the predefined and developed sentiments in the context of predicting returns or volatility.

\subsection{Emotions in financial Market}

\chapter{10-K Fillings Extraction Model}
\label{extraction-model}
In this research, we utilised textual data for financial analysis. Among the myriad kinds of textual data available, we selected Form 10-K fillings, which contain some of the richest information about firms. Specifically, this paper focused on the Item 1A risk factor section of the filling to extract more informative features, while collecting 10-K fillings itself for predicting sentiment scores. Form 10-K fillings are the official documents that all publicly traded firms in the United State are required to submit to the Securities and Exchange Commission(SEC). The SEC enforces very strict rules regarding the content and structure of the information required in Form 10-K fillings. These fillings contain no pictures or charts. A well-structured 10-K is divided into five separate parts. The first three parts offer a concise summary of the firm’s primary business activities, including its services and products; enumerate every risk encountered by the firm; and provide detailed financial information about the firm over the last five years. The fourth part delivers a senior management analysis of its financial results. The final part includes the actual financial figures; the firm’s audited financial statements, which consist of the income statement, balance sheets, and statement of cash flows. A detailed description of the 10-K structure can be found in \hyperref[appendix_10-k]{Appendix A}. Hence, the Form 10-K exhibits uniform structures. Thanks to these organised structures, we can algorithmically extract information on the fillings through \ref{fig:extraction-model}. \cite{Hering2016, Sha2023} 

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.75\linewidth]{ug/images/extraction-model.jpeg}
    \caption{10-K Filling Extraction Model}
    \label{fig:extraction-model}
\end{figure}

\section{10-K Fillings Collection}
Form 10-K fillings can officially be found on the SEC website, where they are available to the public. The SEC provides 10-K fillings in HTML or TXT formats through its database system, known as the Electronic Data Gathering, Analysis, and Retrieval(EDGAR). This system offers various official fillings, including 10-K, 10-Q, 8-K, and 6-K, among others. For our research, we were able to collect most of the 10-K fillings from firms in the technology sector on the EDGAR. EDGAR offers files in both TXT and HTML formats, but since most of the 10-K reports were easily accessible in HTML format, our algorithms focused exclusively on extracting HTML files. For instance, EDGAR has 8140 fillings in HTML format of all firms in the S&P 500, whereas, there are only 48 fillings in TXT format \cite{Sha2023}. We considered the portfolio of Invesco QQQ Trust Series 1 an exchange-traded fund(QQQ or QQQ ETF) to compile a list of tech firms representing the technology sector in the United States. The QQQ is designed to passively track the Nasdaq 100 Index, which conventionally represents the technology sector. More details of the QQQ portfolio can be found in \hyperref[portfolio]{5.3 Portfolio section} 

Form 10-K fillings can be easily collected by using a Central Index Key(CIK)in EDGAR. A CIK is a number given to an individual or company by the SEC, used to identify the ownership of a filling. Initially, we collected the CIKs list of firms listed in QQQ to facilitate data retrieval through the RSS Feeds provided by EDGAR. It is important to consider the crawler’s limitation, which is a maximum request rate of ten requests per second, to ensure equitable access. In this report, we specifically focused on Item 1A, the risk factor, as well as the entirety of the 10-K filling for our purpose. As the mandatory disclosure of Item 1A as a separate section of the filling has been required since 2005, our crawler collected 10-K forms spanning nearly 17 years, from January 2006 to December 2023, for every firm listed in QQQ. In total, we collected 1383 fillings, primarily stored in HTML.


\section{Risk Factor Extraction Process}
In this study, our focus was exclusively on Item 1A, Risk Factor section. Initially, we extracted all contents from the Item 1A Risk Factor section. However, due to the insufficient sample size for analysing a single company’s sentiment score, we adapted our approach. We separately extracted each risk heading along with its corresponding content and then aggregated these elements together.

The Form 10-Ks feature well-structured formats in HTML. We observed the following regularities in the structure of the 10-Ks:
\begin{itemize}
    \item The heading of a risk is followed by the explanations of the risk.
    \item There is a summary of the Risk Factor section at the beginning of the section.
    \item Non-informative elements are located in the footers, including page numbers, copyright information, or disclaimers
    \item Reiterated expressions appear in a certain section; specifically, “Item 1A Risk Factor (Continued).”
    \item Diverse layouts exist within a report, with variations in fonts, headings, and overall formatting.

\end{itemize}
With the aid of the regularity in the filling’s structure in HTML, our algorithms can accurately locate and extract the risk factor section. The process of extracting risk factors from HTML involves four steps: 
\begin{itemize}
    \item Page Footers Removal
    \item Headings Extraction
    \item Risk Factor Section Detection
    \item Titles Extraction
\end{itemize}

\subsection{Page Footers Removal}

There is repetitive information in page footers of HTML files. Initially, we removed “Item IA. Risk Factors(Continued)” by using RegExr 4 \ref{tab:rex}. We observed that page footers are typically found near horizontal lines marked by ‘\textless hr\textgreater’ or ‘\textless div\textgreater’ tags with ‘page-break-after: always’ RegExr 2 \ref{tab:rex}. They were then replaced with a ‘split of pages’ marker by using the BeautifulSoup library. Our algorithm subsequently identified and removed non-informative page footers- both textual and numerical- located near these markers by scanning both forward and backward. 

\subsection{Headings Extraction}

Headings were extracted by detecting tags typically associated with headings, identified by their styles or attributes. The algorithm uses these attributes \ref{tab:rex} to identify headings and encapsulate their contents within ‘ \textless heading\textgreater’ and ‘ \textless /heading\textgreater’ markers. These identified headings are then readily identifiable for further analysis steps. Additionally, we removed the ‘ \textless heading\textgreater’ and ‘ \textless/heading\textgreater’ tags when the heading contained five words or fewer. 

\subsection{Risk Factor Section Detection}

The risk factors section is extracted by identifying the heading “Item 1A - Risk Factor” RegExr 6,7 \ref{tab:rex},  and the subsequent heading RegExr 8-16 \ref{tab:rex}. The contents of the section are located between the heading and the subsequent headings. Typically, if the pattern “Item 1A -Risk Factor” appears in the heading, it may contain extra spaces, line breaks or variations. Conversely, if it is not in the heading, the pattern is consistently enclosed in special quotation marks, either “&ldquo”, “&rdquo”, or “&quot”. The algorithm, improved by \cite{Hering2016}, detects the positions of the several types of headings by iterating through the regex pattern \ref{tab:pattern}. Additionally, after extracting the contents of the section, the algorithm checks if the extracted content typically exceeds 1000 characters in length. 


\subsection{Titles Extraction}

The extracted headings include both titles (such as “Risks Related to Legal, Regulatory and Compliance Matters; FORWARD-LOOKING STATEMENT”) and headings (like “We may be unable to adequately protect our proprietary intellectual property rights, which may limit our ability to compete effectively.”). We noted that titles have all words starting with capital letters, after removing stopwords. Thus, for headings fitting this pattern, we replace their markers with ‘\textless title \textgreater’ and ‘\textless /title \textgreater’.


\begin{table}[ht]
    \centering
    \begin{tabular}{|m{0.5cm}|m{7cm}|m{7cm}|}
        \hline
        \textbf{ID} & \textbf{Description} & \textbf{Regular Expression} \\
        \hline
        \textbf{1} & {\scriptsize Removal of “Continued” pattern in page footer} & {\scriptsize ITEM 1A\.(.{0,10})RISK TORS\(.{0,10}\)(continued)} \\
        \hline
        \textbf{2} & {\scriptsize Search \textless div\textgreater tags with specific style } & {\scriptsize page-break-after\s*:\s*always } \\
        \hline
        \textbf{3} & {\scriptsize Extraction of Item 1A. } & {\scriptsize [ˆ”“]ITEM \ s*1A[ˆ””]} \\
        \hline
        \textbf{4} & {\scriptsize Extraction of Item 1A. (alternative)} & {\scriptsize RISK[ˆa-zA-Z0-9”“]*FACTOR \ s*S[ˆ””] } \\
        \hline
        \textbf{5} & {\scriptsize Extraction of Item 1B.} & {\scriptsize ITEM \ s*1B} \\
        \hline
        \textbf{6} & {\scriptsize Extraction of Item 1B. (alternative)} & {\scriptsize Unresolved*[ˆa-zA-Z0-9]*Staff*[ˆa-zA-Z09]*Comments} \\
        \hline
        \textbf{7} & {\scriptsize Extraction of Item 2.} & {\scriptsize ITEM \ s*2} \\
        \hline
        \textbf{8} & {\scriptsize Extraction of Item 3.} & {\scriptsize ITEM \ s*3}  \\
        \hline
        \textbf{9} & {\scriptsize Extraction of Item 2. (alternative)} & {\scriptsize Legal*[ˆa-zA-Z0-9]*Proceedings} \\
        \hline
        \textbf{10} & {\scriptsize Extraction of special case ’Management’s Report’} & {\scriptsize Management*[ˆa-zA-Z0-9]*S[ˆa-zA-Z09]*Report} \\
        \hline
        \textbf{11} & {\scriptsize Extraction of special case ’Managing Global Risk’} & {\scriptsize Managing*[ˆa-zA-Z0-9]*Global[ˆa-zA-Z09]*Risk} \\
        \hline
        \textbf{12} & {\scriptsize Extraction of special case ’Statement of Income Analysis’} & {\scriptsize Statement*[ˆa-zA-Z0-9]*of[ˆa-zA-Z09]*Income[ˆa-zA-Z0-9]*Analysis} \\
        \hline
        \textbf{13} & {\scriptsize Extraction of special case ’non GAAP Financial Measure’} & {\scriptsize non*[ˆa-zA-Z0-9]*GAAP[ˆa-zA-Z09]*Financial[ˆa-zA-Z0-9]*Measure} \\
        \hline
    \end{tabular}
    \caption{Regular expression for extraction in HTML}
    \label{tab:rex}
\end{table}

\begin{table}[ht]
    \centering
    \begin{tabular}{|l|l|}
        \hline
        \textbf{Sample} & \textbf{Description} \\
        \hline
        {\scriptsize font-weight: bold} & {\scriptsize Bold heading with style attribute} \\
        \hline
        \scriptsize{font-weight: 700} & {\scriptsize Bold heading with style attribute}  \\
        \hline
        \scriptsize{\textless b\textgreater\textless/b\textgreater} & {\scriptsize Bold tag}  \\
        \hline
        \scriptsize{<strong></strong>} & {\scriptsize Strong tag}  \\
        \hline
        \scriptsize{text-decoration: underline} & {\scriptsize Underlined heading with style attribute}  \\
        \hline
        \scriptsize{font-style: italic} & {\scriptsize Italic heading with style attribute} \\
        \hline
        \scriptsize{\textless i\textgreater\textless /i\textgreater} & {\scriptsize Italic tag} \\
        \hline
        \scriptsize{\textless em\textgreater\textless/em\textgreater} & {\scriptsize Emphasized tag} \\
        \hline
        
    \end{tabular}
    \caption{Heading Pattern}
    \label{tab:pattern}
\end{table}




\section{Preliminary Analysis}
The data extraction algorithm successfully collected the 10-K fillings from 94 firms out of 100 firms listed in the QQQ ETF directly from the SEC EDGAR database. The six firms that are not included are foreign-based and instead issue 20-F fillings. In total, the algorithm gathered 1383 fillings, demonstrating its effectiveness for 10-K fillings. However, some documents do not follow the standard regularity, including having an unstructured format or placing the risk factor section in other sections. In these cases, our algorithm was not able to accurately identify and collect the relevant information. Despite this fact, its design allows for adaptability to collect various types of SEC fillings, making it a scalable tool for financial research that requires textual report analysis. 


\chapter{Methodology}
In this section, we introduce the supervised learning model for 10-K sentiment analysis and explain how the model functions. Section 1 and 2, which are referred to as \cite{ke2020predicting}, demonstrate how the model generates a sentiment score of a 10-K filling by using the return at the time of the filling’s publication as a label. In Section 3, we describe in detail the model that uses volatility as a label to estimate sentiment and its adaptation process. In Section 4, to represent the macroscopic sentiment trend of the technology sector, we introduce the Kalman filter and its process for removing noise in the context of 10-K sentiment analysis. 

\section{A Sentiment Prediction Model}

\subsection{Notation}
To establish notation for the probabilistic sentiment model, we considered $n$ as a set of 10-K fillings, and $V$ as a vocabulary consisting of $m$ words. The $i$ = 1,…,$n$ represented the index of 10-K fillings. It represented both the filling’s publication date and the company to which it relates. The word counts were recorded in a vector $d_i \in \mathbb{R}^m_+$
 ,where $d_{i,j}$ represented the number of times word $j$ occurs in 10-K filling $i$. We defined $D \in \mathbb{R}^{n \times m}_+$ as a document-term matrix, with $D = [d_1, ..., d_n]$, representing word counts in each document. $d_i$ was the  $i$-th row of $D$, and the indices of columns were listed in the set S, which was a subset of vocabulary, i.e., $S \subseteq V$. $D_{[S],i}$ was the submatrix of the $i$-th filling. $d_{[S],i}$ was the word count vector in subset $S$ for the $i$-th filling.

We labelled filling $i$ with the associated time series variable $y_i$, either return or volatility, on the publication date of the filling. In this project, we assumed that each filling had a sentiment score, denoted by $p_i \in [0,1]$
. In the case of return fitting, a high score suggested that the filling had a predominantly positive tone in the report. However, it implies neither positive nor negative in the context of volatility fitting, as volatility signifies market uncertainty. Therefore, in the volatility setting, a high score indicates high market uncertainty and a low score suggests low market uncertainty.

\subsection{Model Assumptions}
\subsubsection{Assumption 1}
We assumed that vocabulary V consisted of a set $S$ of sentiment-charged words and a set $N$ of neutral words, i.e., $V = S \cup N$. These sets were mutually exclusive, i.e. $S \cap N = \emptyset
$. Furthermore, we posited the set of sentiment-charged words affected the tone of a filling, whereas the set of neutral words did not influence its tone, i.e, $ d_{[S], i} \not\!\perp\!\!\!\perp
 p_i$ and $ d_{[N], i} \perp \!\!\! \perp p_i$. The sentiment word count was independent of the neural word count, i.e., $d_{[S], i} \perp \!\!\! \perp d_{[N], i}$, implying that the model did not include the neutral words. 
\subsubsection{Assumption 2}
We assumed that the sentiment-charged word counts $d_{[S],i}$ were produced by a mixture multinomial distribution: 

\begin{equation} \label{eu_eqn}
d_{[S],i} \sim \text{Multinomial} \left( s_{i}, p_{i}O^{+} + (1 - p_{i})O^{-} \right)
\end{equation}

,where $s_i$ was the total count of sentiment-charged words in the $i$-th filling, i.e., $s_i = \sum_{w \in S} d_{w,i}$. This determined the scale of the multinomial. We then modelled $O \in \mathbb{R}^{|S| \times 2}_+$ matrix, representing the probabilities of individual word counts using a mixture model that incorporated positive and negative topics. The model included $O_+$, a vector of $|S|$ non-negative elements with a unit $\ell^1$-norm, representing a probability distribution across words, such that $\sum_{}^{|S|} o_{+,w} = 1$
, where $o_{+,w} \in O_+$ and  $w \in |S|$. $O_+$  symbolised a ‘positive sentiment topic’ and represented the expected distribution of word frequencies in a filling with the highest possible positive sentiment, where the probability $p_i$ equals 1. Similarly, $O_-$ symbolised a ‘negative sentiment topic’ that represented the distribution of word frequencies in a filling with the most pronounced negative sentiment, where the probability $p_i$ equals 0. The sentiment score $p_i$, where $0 < p_i <1$, was a mixture coefficient of two sentiment topics.

\subsubsection{Assumption 3}
Finally, we assumed that the sentiment score fully encapsulated the information within a filling that affected the dependent variables, i.e., $y_i|p_i \perp \!\!\! \perp   d_i$. This implied that sentiment score could primarily be used as a feature for return or volatility prediction models. 

\subsection{Model}
The model outlined below, incorporating these three assumptions, consisted of three steps for predicting the sentiment score of a 10-K filling. First, we extracted the set of sentiment-charged words, denoted as $\hat{S_n}$. Second, we estimated the probabilistic distribution matrix of positive-negative topic parameters over words, which is $\hat{O}=[\hat{O_+},\hat{O_-}]$. Third, we predicted the sentiment scores $\hat{p_i}$ of a 10-K filling using penalised maximum likelihood estimation. Each step was described in detail in Sections 4.1.3.1 to 4.1.3.2. The model overview can be found in \ref{fig:sentiment-model}.

\begin{figure}[h!]
    \centering
    \includegraphics[width=1.0\linewidth]{ug/images/sentiment-model.jpeg}
    \caption{A Sentiment Prediction Model}
    \label{fig:sentiment-model}
\end{figure}

\subsubsection{Extracting sentiment-charged words}
In a 10-K filling, sentiment-neutral words were likely to predominate in terms of the number of words and total counts. This dominance tended to introduce noise and could make the extraction of sentiment-charged words from the entire vocabulary computationally burdensome, especially if sentiment-neutral words were not selectively excluded. Hence, in our model, we filtered out the set of sentiment-neutral words and focused solely on the subset of sentiment-charged words, estimating topic parameters for this subset alone. To achieve this, we utilised realised stock returns as a label (volatility was also used as a label, with the fitting process described in Section 4.2 in detail). The indicator function, represented by $\mathbb{1}_{\{a\}}$, was defined as 1 when condition $a$ was met, and 0 otherwise. Consequently, $\mathbb{1}_{\{d_{i,w} > 0\}}$ represented the presence of word $w$ in the $i$-th filling, and $\mathbb{1}_{\{y_i > 0\}}
$ denoted that the return variable associated with the $i$-th filling was positive. For each word $w \in V,$ we could compute the frequency of word $w$ in 10-K fillings with positive returns relative to its overall frequency across all fillings using the following formula: 

\begin{equation} \label{4.2}
f_w = \frac{\sum_{i=1}^{n} \mathbb{1}_{\{d_{w,i}>0\}} * \mathbb{1}_{\{y_i>0\}}}{\sum_{i=1}^{n} \mathbb{1}_{\{d_{w,i}>0\}}}
\end{equation}

This measure signified the word’s sentiment tone to return values. For any given word $w$, if fillings that contained it generally coincide with positive rather than negative returns, $w$ was tagged with a positive sentiment. Conversely, if occurrences of $w$ tended more often to align with negative returns, it was considered to have a negative return. 

Subsequently, we assessed $f_w$ against appropriate thresholds. If $f_w$ was approximately 0.5, this suggested that the word was sentiment-neutral and belonged to the set $N$. To differentiate sentiment-charged words, we defined $\alpha_+$ and $\alpha_-$ within the interval $(0, 0.5]$ to filter out sentiment-neutral words. A word was classified as a positive sentiment word if  $f_w > 0.5+\alpha_+$ and as a negative sentiment word if  $f_w < 0.5+\alpha_-$. We also defined a third threshold, $k \in \mathbb{N}$, which pertains to the count of word $w$ across all fillings. The threshold $k$ acted as a minimum frequency requirement to mitigate the impact of rare words, which could be sources of noise. For instance, a term appearing only once in all fillings would result in an $f_w$ of either 0 or 1, thus being noisy and unreliable itself. By applying the threshold $k$, we filtered out such noisy instances, requiring that a word appeared more than $k$ times to be considered: ${\sum_{i=1}^{n} \mathbb{1}_{\{d_{w,i}>0\}}} > k$. With these conditions, our extracted set $\hat{S}$ was defined by

\begin{equation} \label{4.3}
\hat{S} = \left( \{w : f_w > 0.5 + \alpha^+\} \cup \{w : f_w < 0.5 - \alpha^-\} \right) \cap \left\{ w : \sum_{i=1}^{n} \mathbb{1}_{\{d_{w,i}>0\}} > k \right\}
\end{equation}

\subsubsection{Estimating probabilistic distribution of topic parameters}

In this process, our goal was to estimate the topic parameters for a report. $\hat{O_i} =[\hat{O_i+},\hat{O_-}]$ referred to a 1x2 vector, with each element corresponding to the estimated probabilistic distribution of positive topic or negative topic for a filling, respectively. To obtain $\hat{O} =[\hat{O_+},\hat{O_-}]$, we associated the sentiment expressed in a 10-K filling with stock returns or volatility (Fitting on volatility will be explained on 1.4 section). This approach comes from the assumption that stock returns or volatility at the publication date of a 10-K filling represented the sentiment for it. Note that we did not have direct sentiment scores for the fillings, thus we estimated the sentiment score by using the standardised ranks of stock returns as a proxy in the equation:

\begin{equation} \label{4.4}
\hat{p}_i = \frac{\text{rank}(y_i)}{n},
\end{equation}

where the expression defined $\hat{p}_i$, the estimated sentiment score for the $i$-th filling, as the rank of $y_i$, i.e, a stock return or volatility, divided by $n$, the total number of fillings. The rank of $y_i$ was sorted in ascending order. 

With these estimated sentiment scores, we had a matrix $\hat{W}$ containing two rows for each filling: one for the estimated positive sentiment score $\hat{p_i}$ and one for the estimated negative sentiment score $1-\hat{p_i}$ in the equation:

\begin{equation} \label{4.5}
\hat{W} = \begin{bmatrix}    \hat{p}_1 & \hat{p}_2 & \dots & \hat{p}_n \\    1-\hat{p}_1 & 1-\hat{p}_2 & \dots & 1-\hat{p}_n\end{bmatrix},
\end{equation}

Subsequently, we adjusted the counts of sentiment-charged words by dividing each count by the total sentiment-charged word count for the filling like in the equation:

\begin{equation} \label{4.6}
\hat{h}_i = \frac{d_{\hat{[S]}, i}}{\hat{s}_i}, \text{ where } \hat{s}_i = \sum_{w \in \hat{S}} d_{w,i},
\end{equation}

where $\hat{h_i}$ was the counts of sentiment-charged words. This was defined as the ratio of $d_{\hat{[S]}, i}$, each word count within the subset $[\hat{S}]$ of the $i$-th filling, to $\hat{s_i}$, which was the total sentiment-charged word count for the filling. These relative term counts were collected in a matrix $\hat{H} = [\hat{h_1},...,\hat{h_n}]  $. 

In this process, we aimed to estimate a matrix $\hat{O}$, which contained parameters that referred to the probability distribution of positive and negative sentiments. Then, we could estimate $\hat{O}$ with a regression, using matrix $\hat{H}$ as the predicted outcome and matrix $\hat{W}$ as the predictor like in the equation:

\begin{equation} \label{4.7}
\hat{O} = \hat{H}\hat{W}^T (\hat{W}\hat{W}^T)^{-1}.
\end{equation}

In the final step, to ensure the estimates correspond to a probability distribution, we corrected any negative entries by resetting them to zero and then re-normalised each column so that their totals were equal to one. This process produced a revised matrix, but to simplify notation, we reused $\hat{O}$ for the resulting matrix, and we labelled its first and second columns as $\hat{O_+}$ and $ \hat{O_-}$, respectively. 

\subsection{Scoring New Filings}

Now that we constructed estimators $\hat{S}$  and $\hat{O}$. Also, from the mixed multinomial distribution in 1.2.2, we could estimate the filling’s count vector, which is $d_{[S]}$. Given all estimates $\hat{S}$, $\hat{O}$, and $d_{[S]}$, we could estimate $p$ by maximum likelihood estimation (MLE):

\begin{equation} \label{4.8}
\hat{p} = \underset{p \in [0,1]}{\mathrm{arg\,max}} \left\{ \hat{s}^{-1} \sum_{w \in \hat{S}} d_w \log (\hat{p}\hat{O}_{+,w} + (1 - p)\hat{O}_{-,w}) + \lambda \log(p(1 - p)) \right\},
\end{equation}

where $\hat{s}$ represented the total count of words from the set $\hat{S}$ in the new filling, while $d_w, \hat{O_{+,w}},$ and $ \hat{O_{-,w}}$ referred to the $w$-th elements of their respective vectors, and $\lambda$ was a positive constant used to adjust the model. In the MLE, $\lambda \log(p(1 - p))$ was a penalty term to avoid overfitting when there were reports with few sentiment-charged words. For instance, if the filling had a limited number of positive sentiment-charged words without negative words, the model believed the filling had a positive tone even if the filling just only contained a few positive words actually. The term served as a regularising factor that pushed the estimated sentiments toward 0.5, indicative of a neutral sentiment. In other words, the penalty terms nudged the model to be more conservative when scoring new fillings. 

\section{Volatility Label}
\label{sec:volatility_label}
I explained the sentiment prediction model with the firm’s return as a label, which was referred to by \cite{ke2020predicting}. The author of the model suggested the model was presented as universally adaptable. However, its core assumptions and methodologies might not suit every forecasting objective such as using volatility as a label. While the current model with a return label fits into a binary or discrete framework, volatility does not naturally fit into a binary or discrete framework. Due to that, we should do adaptation to use volatility as a label for the model. 

In the current narrative of returns prediction, we could employ returns as a label corresponding to binary sentiment topics, i.e., positive and negative, because one either made a loss or a profit. However, this classification was not clear in the context of volatility. The adaptation for volatility was to set a threshold $\theta$ and we labelled all values above this $\theta$ point as high volatility and those below it as low volatility. We then replaced 0 by the $\theta$ in \hyperref[4.2]{Equation 4.2} and the value 0.5 by quantile $q$ in \hyperref[4.3]{Equation 4.3}. Note that there were no right threshold or quantile, but we should keep in mind that our choice of hyper-parameter would impact the outcome of the model. 

Volatility is in nature an asymmetric variable, thus getting a ranking of the volatility like in \hyperref[4.4]{Equation 4.4}  will lose substantial informative information to estimate the sentiment score. Subsequently, the normalising volatility can be an appropriate alternative as it preserves asymmetry:

\begin{equation} \label{4.9}
\hat{p}_i^* = \frac{y_i - \min(y)}{\max(y) - \min(y)},
\end{equation}

where $\hat{p}_i^*$ is greater than or equal to 0 and less than or equal to 1. However, the normalised volatility itself was not able to catch the outlier of the market movement despite making it symmetric around zero, leading to poor predictive performance. Thus, we used volatility values over multiple days for the robust labels. In practice, we averaged the volatility over three days. We will discuss more in detail on the \hyperref[sec:hyperparameter]{5.2 hyper-parameter section}. 

\section{Kalman Filter}
In this paper, we generated sentiment metrics of both the technology sector and a single firm(e.g. Nvidia) with 10-K fillings over a certain time. But, the estimation of the industry’s time-varying sentiment measures should be very noisy. To obtain robust sentiment features on time series, we adapted the Kalman filter to smooth the time-varying noisy signals. In the context of smoothing sentiments of 10-K fillings across the industry, we referred to \cite{borovkova2017sector} to adapt the Kalman filter to our context. Following the adapted Kalman filter in our context of work, 
\begin{equation} \label{4.10}
\\\mu_{t+1} = \mu_t + \eta_t, \quad \eta_t \sim \mathcal{N}(0, \sigma_{\eta}^2)
\end{equation}

\begin{equation} \label{4.11}
\\v_t = \mu_t + \epsilon_t, \quad \epsilon_t \sim \mathcal{N}(0, \sigma_{\epsilon}^2)
\end{equation}

Firstly, we assumed that there were the observed sentiment score $\\v_t$ and an unobserved sentiment score - the state variable $\mu_t$ in the Kalman filter framework. We extracted the unobserved sentiment score from the publication-date-based sentiment scores via the first and the second equations. \hyperref[4.10]{The first equation(4.10)} referred to the prediction model and \hyperref[4.11]{the second equation(4.11)} referred to the correction model. The prediction model updated the unobserved state, and the correction model represented the observed sentiment cores that were affected by the state variable. The Kalman filter worked recursively through prediction and correction. It predicted the unobserved state estimates from the previous weighted variable. You can find a more detailed explanation for this Kalman filter model in \cite{durbin2012time}. In our study, we employed the Kalman smoother, instead of the Kalman filter, as the former one was likely to be more proper to show a retrospective trend to analyse the industry-level sentiments. Both the Kalman filter and the Kalman smoother are very similar in terms of estimating the unobserved sentiment score. However, the Kalman filter estimates the unobserved sentiment score at time $t$ given observations up to and including time $t$, whereas the Kalman smoother estimates the hidden sentiment score based on all observations for $t = 1,..., T$. In this paper, we called the Kalman smoother as the Kalman filter for convenience.   

To adapt the Kalman filter in our context, our sentiment scores should be converted to time series data, because there were multiple sentiment scores on the same publication data of 10-K fillings. In order to convert it to time series data, we aggregated sentiment scores on the same date over all firms like $\bar{p}_t = \frac{1}{|A_t|} \sum_{i \in A_t} \hat{p}_i,$ where $A_t$ referred to the set of 10-K fillings published on day $t$. Subsequently, these observed sentiment scores $\bar{p}_t$ were substituted by $v_t$ in \hyperref[4.9]{Equation 4.9} and obtained the filtered industry-level sentiment scores $\bar{p}_t$ after applying the Kalman filter. In other words, this approach implied that all 10-K fillings published on the same day were treated equally. This logic may enable the model to estimate the more accurate sentiment scores, but it may not be realistic. Investors do not invest in firms in equal proportions.


\chapter{Experiment Results}
In this paper, we aimed to infer sentiment scores from 10-K filling with our suggested model. In total, we generated 12 types of sentiment scores for offering sentiment information for financial analysis. 

Firstly, we divided it into three parts, i.e. technology sector, the top 10 firms, and a firm (Nvidia).  Top 10 firms denote the top 10 most invested companies from 1 to 10 listed in QQQ ETF, as the 50 percentile of the QQQ ETF portfolio consists of the top 10 firms. Secondly, for each part, each one has the sentiment score predicted from 10-K filling itself or only Item 1A risk factor. Each one also is predicted with either a return label or a volatility label. For the case of the sentiment prediction for the tech sector and the top 10, we used the Kalman filter to mitigate noise, subsequently showing the industry-level trend of sentiments. Hence, we predicted a total of 12 types of sentiment scores for our financial analysis.

\begin{figure}[h!]
    \centering
    \includegraphics[width=1.0\linewidth]{ug/images/sentiment_metrics.png}
    \caption{A Sentiment Metric}
    \label{fig:sentiment-metrics}
\end{figure}


In this chapter, we outlined the experimental setup. First, we suggested the hyper-parameters for our model experiments. To compare with the outcomes of the model, we selected the baseline model and explained this model in the following sub-chapter. \textbf{WRITE THE OUTLINE AFTER WRITING UP EVALUATION}

\section{Preprocessing}

Through the 10-K fillings extraction model at \hyperref[extraction-model]{Chapter 3}, we finally collected almost all 10-K fillings listed in the QQQ from 2006 to 2023, which is 1383 fillings as well as the text files with only the Item 1A risk factor section extracted. With these files, we preprocessed them before feeding them into our prediction model. Firstly, we split all sentences of a filling into words. Secondly, we removed non-alphabetic tokens from the texts such as numbers, proper nouns, and special characters(i.e. punctuations). For instance, Item 8 of a 10-K filling contains many numbers as a financial statement of a company is included in that section. As numbers were not necessary for textual analysis, we removed them. Thirdly, we removed stop words such as "is", "the", or "and", as they do not contain informative information.  In the final step of the preprocessing, we selected lemmatisation, instead of stemming. Although lemmatisation is computationally more expensive than stemming, it tends to capture the more accurate base form of a word through linguistic analysis (i.e. considering Parts-of-speech) \cite{AnalyticsVidhya2022}.

\section{Hyper-parameter Setting}
\label{sec:hyperparameter}
We set the hyper-parameters equal for the models to ensure a fair comparison between the sentiments with return labels and those with volatility labels for both a technology sector and a firm.

When using a return as our label, we used the publication time $t$ over the days $t - 1$ to $t +1$, as suggested in \cite{ke2020predicting}. Using only the return at the publication time $t$ can produce a noisy signal. A return may slowly respond to the content of 10-K fillings, so the market may need the time to reflect the released 10-K filling of a firm to its stock price. Additionally, some contents of 10-K fillings can already be reflected in its return as the 10-K fillings, annually released, can contain some repetitive contents. To mitigate the noisy signal, we used open-close log returns
$R_t$:

\begin{equation} \label{5.1}
R_t = \log\left(\frac{P_{(t-1)c}}{P_{(t+1)o}}\right),
\end{equation}

where $P$ refers to the equity price at a given time. The market open time at day $t$ denoted $t_O$, and the market close time at day $t$ denoted $t_C$. we used open-close return to make a symmetric alignment with the 10-K filling’s published date.

In the context of the volatility label, we used the intra-daily range among other standard volatility proxies such as squared returns, or the realised volatility as it is a less noisy volatility proxy, leading to less distortion \cite{alizadeh2002range, patton2011volatility}. The intra-daily log range is defined as:

\begin{equation} \label{5.2}
RG_t = \max_{\tau} \log P_{\tau} - \min_{\tau} \log P_{\tau}, \quad \tau \in [t_o,t_c].
\end{equation}

The volatility proxy $\tilde{V_t}$ was then computed like this:
\begin{equation} \label{5.3}
\tilde{V}_t = \frac{RG_t^2}{4\log(2)},
\end{equation}

where the intra-daily log range was squared and divided by the adjustment factor, $\frac{1}{4\log(2)}$. The adjustment factor is used to correct potential bias that may occur in the data generation process when we assume that the process follows Brownian motion with drift Parkinson, 1980. Then, we attained the averaged volatility over three days for a more reliable label:

\begin{equation} \label{5.4}
V_t = \frac{1}{3} \left( \tilde{V}_{t-1} + \tilde{V}_t + \tilde{V}_{t+1} \right),
\end{equation}

which is the common approach to calculating multi-day aggregation of daily volatility proxies \cite{corsi2009simple}.

In \hyperref[sec:volatility_label]{section 4.2}, we replaced 0 by the $\theta$ in \hyperref[4.2]{Equation 4.2} , and the value 0.5 by quantile $q$ in \hyperref[4.3]{Equation 4.3} . $\theta$ is a threshold and we used it to define high and low volatility. To figure out the balanced $\theta $ and $q$ hyper-parameter for both the technology sector and a firm(in this paper, Nvidia), we selected the value of $\theta$ and $q$ from \ref{fig:qqq_vol}, \ref{fig:nvidia_vol} on the training windows. In the technology sector from \ref{fig:qqq_vol}, we practically set $\theta$ as the 65th percentile of the distribution and $q$ as 0.65. It implies we defined the volatility above the 65 quantile as high volatility, whereas the volatility below the 65 quantile is low volatility. Note that we set the 65th percentile and 0.65 for $\theta$ and $q$ for both QQQ volatility itself and the volatilities of the top 10 firms in QQQ, respectively. Empirically, the 3-day volatility for both QQQ and the top 10 showed a similar trend. We can see the peaks from the graph, referring to the financial crisis of 2008 and the COVID-19 pandemic around 2020. In the case of a firm(i.e. Nvidia) from \ref{fig:nvidia_vol}, $\theta $ was set as the 65th percentile of the distribution, and $q$ was 0.65. Similar to the QQQ volatility graph, Nvidia experienced high levels of volatility during the 2008 financial crisis and COVID-19 showed higher volatility movement in general during the training windows.

\begin{figure}[ht]
  \centering
  \begin{minipage}{0.5\textwidth}
    \centering
    \includegraphics[width=\linewidth]{ug/images/QQQ_volatility.png} % Replace with your image path
    \caption{3-Day QQQ Volatility}
    \label{fig:qqq_vol}
  \end{minipage}%
  \begin{minipage}{0.5\textwidth}
    \centering
    \includegraphics[width=\linewidth]{ug/images/nvidia_volatility.png} % Replace with your image path
    \caption{3-Day Nvidia Volatility}
    \label{fig:nvidia_vol}
  \end{minipage}
\end{figure}

Furthermore, returns for both the technology sector and a firm showed a balanced proportion at the median of three-day returns. As such, we set 0.5 at \hyperref[4.3]{Equation 4.3} for both the technology sector in \ref{fig:qqq_ret} and a firm return in \ref{fig:nvidia_ret}. 

\begin{figure}[ht]
  \centering
  \begin{minipage}{0.5\textwidth}
    \centering
    \includegraphics[width=\linewidth]{ug/images/QQQ_returns.png} % Replace with your image path
    \caption{3-Day QQQ Return}
    \label{fig:qqq_ret}
  \end{minipage}%
  \begin{minipage}{0.5\textwidth}
    \centering
    \includegraphics[width=\linewidth]{ug/images/nvidia_returns.png} % Replace with your image path
    \caption{3-Day Nvidia Return}
    \label{fig:nvidia_ret}
  \end{minipage}
\end{figure}

In \hyperref[4.3]{Equation 4.3}, we specifically defined $\alpha$  and $k$. Note that $\alpha$ was a threshold to filter out sentiment-neutral words. We set $\alpha^{+}$ and $\alpha^{-}$ within the interval $(0,0.5]$ such that each of the positive word sets and the negative word sets includes 100 words. Moreover, note that $k \in N$ was another threshold to relate to the count of word $w$ across all fillings such that we used $k$ as a minimum frequency requirement to reduce the influence of rare words. We set $k$ as the 90 percentile quantile of the term frequency distribution. It means we ignored words that appear less than the 90 percentile quantile in a filling. 

In \hyperref[4.8]{Equation 4.8}, we also defined $\lambda$. It was a positive constant and used in a penalty term to adjust our model. The penalty term was used to avoid the model overfitting when few sentiment-charged words appear in the filling. Without the penalty term, the models can consider the filling that contains few negative words but does not include positive as a negative filling. However, 
just because the model contains few negative sentiment-charged words without positive words, that does not mean the filling has a negative tone. To control this phenomenon, we set the penalty coefficient $\lambda$ to 0.1 in the penalty term.

\section{Baseline}
The purpose of the paper was to predict the sentiment score for both a technology sector and a firm from the contents of 10-K fillings. To achieve that, we could infer the sentiment scores for $\hat{p}^{RET}$ and $\hat{p}^{VOL}$ by labelling with return and volatility, respectively. Then, we introduced a baseline sentiment score to compare our sentiment scores with it. 

Our baseline model to calculate baseline sentiment scores was suggested by \cite{Garcia2013}, and used the dictionary created by \cite{LoughranMcDonald2011}. Our baseline Loughran and McDonald(LM) sentiment score,$\hat{p}^{LM}$ , is computed as

\begin{equation} \label{5.2}
\hat{p}_i^{LM} = \left( \sum_{w \in LM^+} d_{i,w} - \sum_{w \in LM^-} d_{i,w} \right) \left( \sum_{w \in V} d_{i,w} \right)^{-1},
\end{equation}

where $LM^{+}$ refers to the positive word lists and $LM^{-}$ refers to the negative words list of Loughran and McDonald’s dictionary. To attain the base sentiment scores, we calculated the difference between the number of positive words and the number of negative words, and then we divided this result by the total number of words in each 10-K filling.   

To explain more of the LM’s dictionary for our robust evaluation, the LM dictionary is traditionally used for financial analysis. LM offered an improved textual dictionary for a better accurate financial analysis in financial documents. Existing the financial dictionary of 10-K fillings based on the Harvard dictionary misclassified the negative words in a financial context. Three-fourths of negative words in the 10-K fillings do not carry a negative connotation in financial reports. To solve this issue, LM suggested three approaches. Firstly, LM created a refined list of words that more accurately reflects negative sentiment in a financial context, by analysing every word that appears in at least 5\% of the SEC’s 10-K fillings. Secondly, LM introduced a term weighting scheme that controls the influence of frequently mentioned words and amplifies the significance of rarer terms, thus mitigating the misclassification of words. Finally, they added five other word classifications(e.g., positive, uncertainty, litigious, strong modal, and weak modal words). They found that these new classifications can be linked to market reactions, volatility in stock returns, unexpected earnings, and trading volumes \cite{LoughranMcDonald2011}. In our study, we used only negative and positive categories to adjust the financial dictionary for our model.

\section{Portfolio}
\label{portfolio}
In this study, we formed a portfolio to evaluate the technology sector sentiment scores. A portfolio can be changed to any portfolio for financial analysis. In practice of our study, we selected Invesco QQQ Trust Series 1 an exchange-traded fund (QQQ or QQQ ETF). This passive fund(i.e., our portfolio) tracks the Nasdaq 100 Index, which consists of shares from 100 of the largest and most innovative non-financial firms listed on the Nasdaq stock exchange. Holdings in QQQ are predominantly in large-cap technology firms, accounting for 60\% of the portfolio. As such, the QQQ is conventionally considered as a technology sector fund. The top 10 holdings represent a 50\% allocation of the portfolio, with 9 out of 10 firms being in the tech sector. To represent the technology sector in the US, we formed two portfolios from the QQQ fund. Firstly, we formed the portfolio, which has the exact same allocation proportion as the QQQ fund itself as of 2023. This allocation proportion is annually corrected so that our current portfolio reflects 2023 allocation data.  The actual 2023 portfolio allocation can be found in \hyperref[appendix_qqq]{Appendix B}. This portfolio is used to compare the sector sentiment scores.

The second portfolio was constructed with the top 10 firms, considering the asset allocation ratio of the first portfolio. In other words, the second portfolio consisted of the top 10 firms, accounting for 50\% of the QQQ portfolio, according to the proportion invested in the first portfolio. This portfolio was computed as:

\begin{equation} \label{5.3}
A_j = \frac{w_j}{\sum_{j=1}^{10} w_j}
,
\end{equation}


where j denoted a firm invested at the $j$-th ranking in the 2023 portfolio \hyperref[appendix_qqq]{Appendix B}, and $w_j$ represented the portfolio weight of the $j$-th firm. $A_j$ referred to the allocated proportion of the $j$-th firm in the 2023 portfolio. 

In the case of a single firm evaluation, we did not form a portfolio for it. Instead, we followed the firm’s stock market price.

\section{Analysis and Evaluation}

\subsection{Correlation Analysis}
\label{correlation}
We used the Pearson correlation to evaluate our estimated sentiment scores. The Pearson correlation coefficient has two assumptions\cite{PearsonCorrelationAssumptions2023}. The first assumption is two variables are continuous. Secondly, two variables are assumed to have a linear relation. Based on these assumptions, we calculated four variables(i.e. $P^{RET}, P^{VOL}, P^{LM},$ and $ Stock $). We could analyse our sentiment scores with the Pearson correlation with four variables. The correlation of variable pairs was calculated through the Pearson correlation formula in \hyperref[pearson-formula]{Appendix C}, and their linear correlation could be checked through a significance test \cite{NCLHypothesisTesting}. In a significance test, 0.05 is conventionally used as a significance level. However, a long list of scientists recently proposed a new p-value threshold(i.e. 0.005) to improve the reproducibility as the conventionally used threshold leads to a high rate of considerable false positives, even without taking into account any issues related to the experiment, procedure, or documentation \cite{Benjamin2017Redefine}. In the practice of our study, we selected both 0.05 and 0.005 thresholds to increase our experiment's reliability. 

\\

\subsection{Most Influential Words}
In the process of predicting sentiment scores for all models we have introduced so far, we could extract the top 15 influential words from $\tilde{p}^{RET}$, and  $\tilde{p}^{VOL}$for each model.
The detailed extraction process can be found in \hyperref[4.3]{Equation 4.3}
\subsection{Technology Sector}
In a technology sector analysis, we computed the sector’s sentiment scores with all firms’ filling or the top 10 firms’ fillings, additionally considering 10-K filling itself or only the Item 1A section, respectively. We assumed the top 10 firms can represent the technology sector with less noise as it accounts for 50\% of the QQQ portfolio. Note that our study wanted to show the trend of the sentiments for the entire technology industry. To improve readability, we introduced a table at the top right for all the score graph figures. 

Moreover, we employed a Kalman filter to represent a reliable sentiment trend for a sector analysis \cite{durbin2012time}. An industry-level analysis generated a considerable amount of signal noise. We could control these noisy signals by using the Kalman filter.

\subsubsection{Sector Correlation Analysis}

\subsubsubsection{\textbf{The Sector Sentiment Model with 10-K filling} (\ref{fig:all_qqq})}

This figure \ref{fig:all_qqq} showed the technology sector’s predicted sentiment scores. These scores were estimated with almost all firms’ 10-K fillings listed in the QQQ(i.e. 97 firms out of 100), and we used 10-K fillings itself to calculate the scores. Note that the sentiment scores labelled with return and volatility in the figure are filtered. When you can see these graphs (\ref{fig:all_qqq_ret_filter}, \ref{fig:all_qqq_vol_filter}), both unfiltered return sentiment and volatility sentiment were significantly noisy. Thus, we selected the filtered ones to show the sector’s trend more reliably. Other models, which will be introduced in the following parts, also used the filtered one. Two graphs(I.e. RET and VOL) in the figure \ref{fig:all_qqq} were filtered one. We calculated the average loss between the estimated sentiment score(i.e. $\tilde{p}^{RET}$ or $\tilde{p}^{VOL}$) and the normalised rank of dependant variable(i.e. $return$ or $volatility$) during the same windows. In other words, we had two kinds of loss. The first loss was the average loss between $\tilde{p}^{RET}$ and the normalised rank of $return$ during the same windows. The second loss was the average value between $\tilde{p}^{VOL}$ and the normalised rank of $volatility$ during the same windows. Also, we calculated how well a sentiment analysis model can predict the return or volatility of the financial market based on the content of 10-K fillings. For this model \ref{fig:all_qqq} showing the sector sentiment trends with filling itself, the loss of the model $\tilde{p}^{RET}$ was 0.25, and the accuracy rate was 78\%. It shows $\tilde{p}^{RET}$was strongly well predicted compared to the $return$ given a window. It means the $\tilde{p}^{RET}$ represented the sentiment of the technology sector for the $return$ given a window. Additionally, the $\tilde{p}^{VOL}$ sentiment of this model showed stronger prediction accuracy. The $\tilde{p}^{VOL}$ model performed a 92\% accuracy rate with 0.22 loss. We could figure out that this model correctly predicted the sector’s sentiment scores. 
When we examined the $\tilde{p}^{LM}$score, it had steadily decreased until around 2018. Then, it considerably and gradually dropped after around 2018, although there were oscillated movements due to noise. In general, the model showed movements in a negative direction even when the technology sector was rapidly soaring around 2018 to before the COVID-19 pandemic. This was because the LM dictionary used to calculate $\tilde{p}^{LM} $ was mostly filled with negative vocabulary. The negative vocabulary was 2,355 and the positive vocabulary was 354 out of 86,531 vocabulary words. The rest of the words(i.e. 83,822) were neutral vocabulary, and we did not need them for our sentiment analysis. Hence, the LM dictionary, whose negative vocabulary constituted around 85\% of the total vocabulary, could imply that the $\tilde{p}^{LM} $was able to tend to move in a negative direction.  Additionally. the sector sentiment prediction model took 37 seconds. 

These tables (\hyperref[tab:all_qqq_corr1]{5.1}, \hyperref[tab:all_qqq_corr2]{5.2}) represented Pearson correlation coefficients between the sentiment estimates including QQQ’s stock price, both filtered and unfiltered. When we examined the table ((\hyperref[tab:all_qqq_corr1]{5.1}), we found that almost all pairs of unfiltered sentiment scores did not show any linear correlation except for the $VOL$ and the $LM$ pair. However, filtered ones showed differently. The filtered ones showed a more meaningful correlation. Almost all sample pair results showed weak, but correlation except for the $LM$ and $Stock$ pair outcome. The exception pair showed a strong negative correlation. The $r$=-.245 from $\tilde{p}^{RET}$ and $\tilde{p}^{VOL}$ could be interpreted as a weak negative correlation \cite{NCLCorrelation}. When the sentiment of $RET$ tended to be positive, the sentiment of $VOL$  tended to be negative(and vice versa). Also, the $r$=+.296 from $\tilde{p}^{RET}$ and $Stock$ showed a weak positive correlation. It could be interpreted that when the sector sentiment of $RET$ could be positive, QQQ stock price tended to rise(and vice versa).  When we examined the figure \ref{fig:all_qqq}, both $\tilde{p}^{RET}$ and $\tilde{p}^{VOL}$ still showed oscillations due to noise from a technology sector, but $\tilde{p}^{RET}$ seemed to increase even under fluctuations during a given window slowly. Especially, it was surprised that the sentiment movement in 2023 was strongly similar to that of the QQQ stock in 2023. The case of $\tilde{p}^{VOL}$ and $Stock$ ($r$=+.121) showed also a positive correlation, but weaker. Moreover, both $r_{\tilde{p}^{RET},\tilde{p}^{LM}}$(=-.359) and $r_{\tilde{p}^{VOL},\tilde{p}^{LM}}$(=-.173) showed a weak negative correlatoin. 

Furthermore, in both tables (\hyperref[tab:all_qqq_corr1]{5.1}, \hyperref[tab:all_qqq_corr2]{5.2}), all pairs showed the lower p-value(\hyperref[tab:all_qqq_corr2]{5.2} showed a far lower p-value), suggesting that the sample results(i.e. the value of the correlation coefficients) offered enough evidence that the null hypothesis could be rejected from the entire population \cite{MinitabBlog2014}.  In our case, the null hypothesis was that there was no correlation between the pair, whereas the alternative hypothesis was that there was a correlation between them. However, the lower p-value of all pairs does not measure the probability that the alternative hypothesis is true. The p-value is not an absolute index for arguing that a hypothesis is true. Instead, the lower p-value shows our experiments have statistical significance \cite{FrostPValues},\cite{ASAStatement2016}, \cite{Park2016}.

\begin{figure}[p]
\centering
\begin{minipage}{0.90\textwidth}
    \centering
    \includegraphics[width=1.0\linewidth]{ug/images/all_qqq.jpeg}
    \caption{The Sector Sentiment Model with 10-K filling}
    \label{fig:all_qqq}
\end{minipage}%
\hfill
\vspace{30pt} % Adjust the space as needed
\begin{minipage}{0.9\textwidth}

    \begin{minipage}[p]{0.9\textwidth}
    \centering    
    \begin{tabular}{lcccc}
    \label{tab:all_qqq_corr1}
    $r_\hat{p}_i,\hat{p}_j$       & RET       & VOL       & LM        & Stock    \\ \hline
    RET    & 1  & ^{*}0.045  & ^{*}$-$0.049 & ^{*}$-$0.001 \\
    VOL    & ^{*}0.045  &  1  & ^{**}$-$0.225 & ^{*}0.040  \\
    LM    & ^{*}$-$0.049 & ^{**}$-$0.225 & 1  & ^{**}$-$0.270 \\
    Stock  & ^{*}$-$0.001 & ^{*}0.041  & ^{**}$-$0.270 & 1  \\ \hline
    \end{tabular}
    \medskip
    $\textit{Note}: ^{*}p$-$value<0.05, ^{**}p$-$value<0.005$
    \captionof{table}{Unfiltered, A Sector Sentiment Correlation with 10-K Filling}
    
    \end{minipage}%
    \hfill

    \begin{minipage}[p]{0.9\textwidth}
    \centering
    \begin{tabular}{lcccc}
    \label{tab:all_qqq_corr2}
    $r_\tilde{p}_i,\tilde{p}_j$      & RET       & VOL       & LM        & Stock    \\ \hline
    RET    & 1  & ^{**}$-$0.245  & ^{**}$-$0.359 & ^{**}0.296 \\
    VOL    & ^{**}$-$0.245  & 1  & ^{**}$-$0.173 & ^{**}0.121  \\
    LM    & ^{**}$-$0.359 & ^{**}$-$0.173 & 1  & ^{**}$-$0.910 \\
    Stock  & ^{**}0.296 & ^{**}0.121  & ^{**}$-$0.910 & 1  \\ \hline
    \end{tabular}
    \medskip
    $\textit{Note}: ^{*}p$-$value<0.05, ^{**}p$-$value<0.005$
    \captionof{table}{Filtered, A Sector Sentiment Correlation with 10-K Filling}
    \end{minipage}

\end{minipage}
\end{figure}

\begin{figure}[p]
  \centering
  \begin{minipage}{0.5\textwidth}
    \centering
    \includegraphics[width=\linewidth]{ug/images/all_qqq_ret_filter.png}
    \caption{\small Unfiltered Sector ${p}^{RET}$ with 10-K}
    \label{fig:all_qqq_ret_filter}
  \end{minipage}%
  \begin{minipage}{0.5\textwidth}
    \centering
    \includegraphics[width=\linewidth]{ug/images/all_qqq_vol_filter.png} 
    \caption{\small Unfiltered Sector ${p}^{VOL}$ with 10-K}
    \label{fig:all_qqq_vol_filter}
  \end{minipage}
\end{figure}

\subsubsubsection{\textbf{The Sector Sentiment Model with Only-Risk-Factor}} (\ref{fig:risk_qqq})

This figure \ref{fig:risk_qqq} showed the technology sector’s predicted sentiment scores. These scores were predicted based on only the risk factor disclosure of almost all firms listed in the QQQ. Note again that figure \ref{fig:risk_qqq} only showed the filtered one (the unfiltered can be found at \hyperref[appendix_risk_qqq]{Appendix D}). The $\tilde{P}^{RET}$ model accuracy was 73\% with the 0.25  loss. It represented $\tilde{P}^{RET}$ was well predicted with only risk factor disclosure compared to the $return$ given a window. The risk factor $\tilde{P}^{RET}$ model accuracy is 5\% lower than the 10-K itself $\tilde{P}^{RET}$ model, which is 78\%. Moreover, the risk factor $\tilde{P}^{VOL}$ model showed a stronger model performance. It performed 90\% accuracy with a 0.22 loss. Compared to the 10-K itself $\tilde{P}^{VOL}$model, the risk factor model showed a 2\% lower accuracy rate. It indicated that $\tilde{P}^{VOL}$ was still very well predicted with only the risk factor section compared to the $Volatilty$  given a window. The $\tilde{p}^{LM}$ showed a gradual decrease after a steep drop at the beginning of 2006 with oscillated noise. It implied that the $\tilde{p}^{LM}$ score was calculated with the LM where negative words predominated and a risk factor section whose tone itself is pessimistic \cite{campbelletal2014a}, \cite{Filzen2015}.

When comparing the number of words used to train for both models(\ref{fig:all_qqq}, \ref{fig:risk_qqq}), the aggregated word counts of all QQQ firm’s fillings(5.6) is 15,863 after the preprocessing process, and that of the risk factor sections of all QQQ firm’s filling(\ref{fig:risk_qqq}) is 9,030 after the same preprocessing process. We could see that the risk factor of all firms contains significant words, while the section usually only takes up 10\% to 15\% of the entire filling. It showed that a risk factor section could be informative for textual analysis in the finance domain. Also. the small difference between the models’ accuracy could support that the risk factor section contains informative textual context to predict the sentiment score with both $return$ label and $Volatility$  label. However, when we examined both models(\ref{fig:all_qqq}, \ref{fig:risk_qqq}), we found that all scores of model \ref{fig:risk_qqq} showed lower scores compared to model \ref{fig:all_qqq}. It indicated that the tone of a risk factor section is pessimistic/negative as other studies also empirically found \cite{campbelletal2014a}, \cite{Filzen2015}. The training time for this model, additionally, was also reduced by 40\%(i.e. 23s)

The table \hyperref[tab:risk_qqq_corr]{5.3} showed the Pearson correlation coefficients of the filtered sentiment sector model with only the risk factor. Compared to the sector model with 10-K filling itself, shown in the table \hyperref[tab:all_qqq_corr2]{5.2}, the sector risk factor model showed, in general, lower $r$ values for all pairs, and some pairs showed altered sign such as the pair of $\tilde{P}^{RET}$and $\tilde{P}^{LM}$, the pair of $\tilde{P}^{RET}$ and $Stock$, and the pair of $\tilde{P}^{VOL}$ and $\tilde{P}^{LM}$. Except for the pair of $\tilde{P}^{RET}$and $\tilde{P}^{LM}$ without showing statistical significance, the $r$=-0.376 from $\tilde{P}^{RET}$ and $Stock$ showed a weak negative correlation in the risk factor model. The more positive tone the sector has, the lower the price the sector has. This outcome was in contrast to that of the model \hyperref[tab:all_qqq_corr2]{5.2}, and also our intuition. Other pairs did not show a meaningful correlation as they were close to 0.

\begin{figure}[p]
\centering
\begin{minipage}{0.90\textwidth}
    \centering
    \includegraphics[width=1.0\linewidth]{ug/images/risk_qqq.jpeg}
    \caption{The Sector Sentiment Model with Only-Risk-Factor}
    \label{fig:risk_qqq}
\end{minipage}%
\hfill
\vspace{10pt} % Adjust the space as needed
\begin{minipage}{0.9\textwidth}
    \begin{minipage}[p]{0.9\textwidth}
    \centering
    \begin{tabular}{lcccc}
    \label{tab:risk_qqq_corr}
    $\tilde{p}$      & RET       & VOL       & LM        & Stock    \\ \hline
    RET    & 1  & ^{**}$-$0.167  & 0.008 & ^{**}$-$0.376 \\
    VOL    & ^{**}$-$0.167   & 1  & ^{**}0.183 & ^{*}0.056  \\
    LM    & 0.008 & ^{**}0.182 & 1  & ^{**}$-$0.764 \\
    Stock  & ^{**}$-$0.376 & ^{*}0.056  & ^{**}$-$0.764 & 1  \\ \hline
    \end{tabular}
    \medskip
    $\textit{Note}: ^{*}p$-$value<0.05, ^{**}p$-$value<0.005$
    \captionof{table}{Filtered, A Sector Sentiment Correlation with Only-Risk-Factor}
    \end{minipage}
\end{minipage}
\end{figure}

\subsubsubsection{\textbf{The Top10 Sentiment Model with 10-K Filling} (\ref{fig:all_top10})}

This figure \ref{fig:all_top10} indicated the top 10 firms’ predicted sentiment scores. These scores were estimated based on the top 10 firms’ 10-K filling itself for a given window from 2006 to 2023.  The $\tilde{p}^{RET}$ model performed a 76\% accuracy rate with 0.22 loss. It showed that the $\tilde{p}^{RET}$ model predicted the sector’s sentiment well compared to $return$ given the window.  The previously mentioned models(\ref{fig:all_qqq}, \ref{fig:risk_qqq}) had 78\% and 72\%, respectively. The $\tilde{p}^{VOL}$ model decreased its accuracy rate to 78\% with the 0.20 loss (\ref{fig:all_qqq}: 92\%, \ref{fig:risk_qqq}: 90\%), but it still predicted the sector’s sentiment labelled with $volatility$ well. The training time for this model was reduced to 13 seconds as we only used the 10-K fillings of the top 10 firms. A total of 7,040 words were used to train this model. 

Compared to the filtered sector’s sentiment score(\ref{fig:all_qqq}), the filtered model(\ref{fig:all_top10}) showed a significantly far clearer trend in the technology sector while noisy signals were removed (the unfiltered model can be found at \hyperref[appendix_all_top10]{Appendix D}). Furthermore, we found extremely interesting a few phenomena in this model. Firstly, we observed the $\tilde{p}^{RET}$ strongly mirrored the top10 stock price movement, even though the $\tilde{p}^{RET}$ had risen steadily from around 2021 to 2024. We could see that the $\tilde{p}^{RET}$ had a steady and moderate increase from 2006 to around 2016, as a similar movement of the top 10 stock was shown. Then, beginning around 2018, the QQQ rose sharply over a few years and showed a steep drop from 2021 to the beginning of 2023. Then, it increased sharply again. The steep drop from 2021 to 2023 could be steamed from the COVID-19 pandemic. Several studies argued that quantitative easing(QE) seemed to be significantly and positively related to a higher recovery of the USA’s stock market. This could explain the QQQ’s sharp increase after the outbreak of the covid-19, \cite{Gagnon2010},\cite{Chen2011},\cite{Curdia2013},\cite{Gilchrist2013},\cite{Wang2019},\cite{Sunder2021},\cite{Seven2021},\cite{Gourinchas2021}. Likewise, our predicted $\tilde{p}^{RET}$sentiment score showed a rapidly sharp rise, but even after the outbreak of the COVID-19. This increase in sentiment momentum post-COVID-19 could imply that this $\tilde{p}^{RET}$ model could not reflect the degree of importance of events, which should be mentioned in 10-K filling with a high probability. This could be because the firm’s managers also did not know how critical an event was at that time when the 10-K filling was written. Also, even if the manager could notice the importance of the event, the model itself could not consider what proportion of the event is attributable to the sentiment. In other words, a negative event or its relevant words, such as COVID-19 or restrictions(i.e. social distancing restrictions), is merely a textual word to our model.   

Furthermore, the $\tilde{p}^{VOL}$ model showed interesting points. Note that a higher $\tilde{p}^{VOL}$ score referred to higher volatility(i.e. the market uncertainty is high), a lower $\tilde{p}^{VOL}$ meant lower volatility and a lower market uncertainty. The $\tilde{p}^{VOL}$ hugely increased until 2010-2011 and gradually dropped until around 2016 to 2018. It implied that the steep rise of the volatility during the period from 2006 to 2010 might be impacted by the 2007-2008 financial crisis. The following gentle decrease of the market uncertainty could be explained by some effects of QE as well. Most economists believed that the Federal Reserve’s fiscal stimulus policy was helpful in rescuing the U.S. and the global economy from the 2007-2008 financial crisis,\cite{Gagnon2010},\cite{Chen2011},\cite{Stein2012},\cite{Gilchrist2013},\cite{Curdia2013},\cite{Wang2019},\cite{Luck2019}. The similar volatility movement happened again at the outbreak of the COVID-19.  Secondly, this model indicated a decalcomania-like movement with $\tilde{p}^{LM}$. When the $\tilde{p}^{VOL}$ rose, the $\tilde{p}^{LM}$ fell with a similar pattern (and vice versa). However, the $\tilde{p}^{LM}$ movements implied that it did not reflect the negative events such as the 2008 crisis or COVID-19 even if it showed a gradual decrease from around 2018. We observed that the gradient during the period from 2018 to 2024 did not seem to be altered. If the $\tilde{p}^{LM}$ model had reflected, the gradient after the outbreak of COVID-19 could have shown a steep drop. Moreover, it could not reflect the boom time for the technology market that occurred from around the beginning of 2020. It implied that the $\tilde{p}^{LM}$ model could not reflect the market situation dynamically. This was because the model was predominantly filled with negative words.

The table \hyperref[tab:all_top10_corr]{5.4} showed the Pearson correlation coefficients of the filtered top10 sentiment model with 10-K filling itself. All $r$ values indicated a strong correlation with rigorous statistical significance. As we could examine previously in the graph \ref{fig:all_top10}, the case of the top 10 firms also showed far clearer correlations as noise was removed while excluding the rest of the 90 firms. The $r$=+.905 from $\tilde{p}^{RET}$ and $\tilde{p}^{VOL}$ represented a strong positive correlation. It implied that the more positive the sector is, the more uncertain the sector becomes. Moreover, $r_{\tilde{p}^{RET}, Stock}$(=+.956) indicated when the sector’s stock price got to increase, the sector’s sentiment became positive(and vice versa). Also, there was a strong positive correlation between $\tilde{p}^{VOL}$ and $Stock$, representing that the stock price rose when the technology market was uncertain(and vice versa). Both $r_{\tilde{p}^{RET},\tilde{p}^{LM}}$(=-.922) and $r_{\tilde{p}^{VOL},\tilde{p}^{LM}}$(=-.903) showed a strong negative correlation. When $\tilde{p}^{LM}$decreased, the market sentiment increased while the market uncertainty rose together(and vice versa). 

\begin{figure}[p]
\centering
\begin{minipage}{0.90\textwidth}
    \centering
    \includegraphics[width=1.0\linewidth]{ug/images/all_top10.jpeg}
    \caption{The Top10 Sentiment Model with 10-K Filling}
    \label{fig:all_top10}
\end{minipage}%
\hfill
\vspace{30pt} % Adjust the space as needed
\begin{minipage}{0.9\textwidth}

    \begin{minipage}[p]{0.9\textwidth}
    \centering
    \begin{tabular}{lcccc}
    \label{tab:all_top10_corr}
    $\tilde{p}$      & RET       & VOL       & LM        & Stock    \\ \hline
    RET    & 1  & ^{**}0.905  & ^{**}$-$0.922 & ^{**}0.956 \\
    VOL    & ^{**}0.905   & 1  & ^{**}$-$0.903 & ^{**}0.824  \\
    LM    & ^{**}$-$0.922 & ^{**}$-$0.903 & 1  & ^{**}$-$0.841 \\
    Stock  & ^{**}0.956 & ^{**}0.824  & ^{**}$-$0.841 & 1  \\ \hline
    \end{tabular}
    \medskip
    $\textit{Note}: ^{*}p$-$value<0.05, ^{**}p$-$value<0.005$
    \captionof{table}{Filtered, The Top10 Sentiment Correlation with 10-K Filling}
    \end{minipage}

\end{minipage}
\end{figure}

\subsubsubsubsection{\textbf{The Top10 Sentiment Model with Only-Risk-Factor}  (\ref{fig:risk_top10})}

This figure \ref{fig:risk_top10} showed the top 10 firms’ estimated sentiment scores by training only the risk factor section. Compared to the top 10 models with 10-K filling(\ref{fig:risk_qqq}), this model performance was decreased slightly to 71\% with the 0.23 loss ($\tilde{p}^{RET})$ and 74\% with the 0.21 loss ($\tilde{p}^{VOL}$). This could be because of the smaller size of the vocabulary of the risk factor than 10-K fillings itself. Then, the training time also was reduced to 8 seconds. When we see this model compared to model \ref{fig:risk_qqq}, we observe that the $\tilde{p}^{RET}$ model steadily increased. However, this model did not reflect the market situation well. This model gradually increased during the technology industry boom time and very slightly decreased after the outbreak of COVID-19. Also, the $\tilde{p}^{VOL}$model showed the market uncertainty steadily fell during the same period. We could observe that the $\tilde{p}^{RET}$ model reflected a market dynamically, but very slightly, whereas the $\tilde{p}^{VOL}$ did not seem to reflect a market well. This was because the model dealt with only the risk factor section where a negative tone was predominant. We should keep in mind the model trained with risk factors had a pessimistic view on the market when evaluating the market with this model. Furthermore, the model for $\tilde{p}^{LM}$ showed the market sentiment gradually and moderately became negative. We could observe that the $\tilde{p}^{LM}$ model could not reflect a market situation dynamically at all. We observed a similar pattern in the previous $\tilde{p}^{LM}$ model(5.10). The model’s negative dominance without dynamic market consideration distorted the technology sector analysis. 

When examining the Pearson correlation table \hyperref[tab:risk_top10_corr]{5.5}, the correlations including $\tilde{p}^{VOL}$ showed differently compared to the top 10 sector model(See the table \hyperref[tab:all_top10_corr]{5.4}). We observed that all correlation, such as$r_{\tilde{p}^{VOL},\tilde{p}^{RET}}$(=+.145), $r_{\tilde{p}^{VOL},\tilde{p}^{LM}}$(=-.349), and $r_{\tilde{p}^{VOL},Stock}$(=+.185), became to have a weak linear correlation. $r_{\tilde{p}^{VOL},\tilde{p}^{RET}}$ and $r_{\tilde{p}^{VOL},Stock}$ even were close to zero, implying that they lost their correlation. We could assume that $\tilde{p}^{VOL}$ score was not calculated correctly in the model trained with the risk factor section. 

\begin{figure}[p]
\centering
\begin{minipage}{0.90\textwidth}
    \centering
    \includegraphics[width=1.0\linewidth]{ug/images/risk_top10.jpeg}
    \caption{The Top10 Sentiment Model with Only-Risk-Factor}
    \label{fig:risk_top10}
\end{minipage}%
\hfill
\vspace{30pt} % Adjust the space as needed
\begin{minipage}{0.9\textwidth}

    \begin{minipage}[p]{0.9\textwidth}
    \centering
    \begin{tabular}{lcccc}
    \label{tab:risk_top10_corr}
    $\tilde{p}$      & RET       & VOL       & LM        & Stock    \\ \hline
    RET    & 1  & 0.145  & ^{**}$-$0.806 & ^{**}0.893 \\
    VOL    & 0.145   & 1  & ^{**}$-$0.349 & ^{*}0.185  \\
    LM    & ^{**}$-$0.806 & ^{**}$-$0.349 & 1  & ^{**}$-$0.937 \\
    Stock  & ^{**}0.893 & ^{*}0.185  & ^{**}$-$0.937 & 1  \\ \hline
    \end{tabular}
    \medskip
    $\textit{Note}: ^{*}p$-$value<0.05, ^{**}p$-$value<0.005$
    \captionof{table}{Filtered, The Top10 Sentiment Correlation with Only-Risk-Factor}
    \end{minipage}

\end{minipage}
\end{figure}

\subsubsection{Sector Most Influential Words}

\subsubsubsection{\textbf{The Sector Sentiment Model with 10-K Filling} (\ref{fig:all_qqq})}
\label{word_all_qqq}
\\
This model \ref{fig:all_qqq} used a total of 15,863 words to predict its sentiment scores. The $\tilde{p}^{RET}$sentiment score model used 55\% of the total words(i.e. 8725 tokens) as 45\% was made up of neutral words. The $\tilde{p}^{VOL}$ sentiment score model employed 60\% of the total words(i.e. 9518 tokens) as 40\% were neutral words. 

\begin{align*}
\tilde{S_{+}}^{RET} &= \textit{ underpay, secondly, dash, musk, incisive, memoranda, stance,} \\
               &\quad \textit{maximizer, bolivia, torque, gilt, div, multifaceted, searcher, formulaic} \\
\tilde{S_{-}}^{RET} &= \textit{transformer, scalar, tango, analgesic, invocation, carney, coconut,} \\
               &\quad \textit{spectral, heath, rigorously, reimagine, infer, tera, markman, enclosure}
\end{align*}

From the positive influential words extracted, we could categorise a few themes that could impact the technology sector’s positive sentiment tone. The first theme can be ‘Innovation and Development’. The words such as \textit{musk, maximiser, searcher, multifaceted}, and \textit{incisive} could be interpreted as innovation-relevant vocabulary. \textit{Maximiser} and \textit{searcher} could refer to a figure to lead technology innovation or development. Interestingly, the word, \textit{musk}, seemed to indicate “Elon Musk”, who is one of the figures to represent innovation under the fourth industrial revolution area. Moreover, the words(i.e. \textit{multifaceted, incisive}) could indicate a capability or capacity(or both) required for innovation and innovative developments under the fourth industrial revolution. As the data showed \ref{fig:all_qqq}, the technology sector has been gradually and significantly developed during the period. This remarkable development could be caused by innovations. In this context, words such as \textit{musk, torque,} and \textit{bolivia} were additionally associated with innovation. But, specifically these words indicated an electric car or robotics. Elon Musk is the CEO of Tesla, which is an American multinational automotive and clean energy firm. Tesla not only develops electric cars but also robots. The \textit{torque} word could be related to an electric vehicle or robot. \textit{Bolivia} is home to the world’s largest lithium deposits \cite{chan2023global}. Lithium is a key component of electric car batteries. Furthermore, the words, including \textit{gilt}, \textit{div}(we interpreted as dividend), \textit{memoranda}, and \textit{stance}, could be interpreted as finance-relevant terms. Good financial health could be an important factor in economic growth \cite{koijen2016financial}. Hence, these finance terms represent 'Financial Health'. The emphasis on improving the financial health of each tech firm might impact their stock positively. 

There were a few themes we could infer from the negative impactful words. 'Technological Difficulty' could be a theme that makes the sector tone negatively. The words, such as \textit{tango, transformer, scalar, infer}, and \textit{tera} could represent a firm’s technological difficulties. For instance, \textit{Tango} was Google’s Augmented Reality deprecated project due to its technological issue \cite{kastrenakes2017google}, \cite{donfro2017google}. Furthermore, \textit{transformer} is a significant natural language process(NLP) model architecture for generative AI. \textit{scalar}, an element used to define a vector space in machine learning, can refer to the number of parameters for an AI model. \textit{Tera} referred to a terabyte, representing a large dataset. Google’s Gemini glitches cost Google 90 billion dollars in stock loss in a single day. The second theme could be ‘Innovation Pressure’ from words, including \textit{reimagine, infer}, and \textit{rigorously}. They could suggest the need for reevaluation, insight, or creativity for a firm to lead innovations, but this innovation pressure could hinder their sentiment.

The following top 15 words are the most influential words in the $\tilde{p}^{VOL}$ sector sentiment score prediction, positively or negatively, respectively. 


\begin{align*}
\tilde{S_{+}}^{VOL} &= \textit{lancet, forearm, renter, koa, fang, irreversibly, granularity, bully} \\
               &\quad \textit{killing, impropriety, misrepresentation, reimportation, amenable, sayer, spoof} \\
\end{align*}

We could infer three themes from the words extracted to increase market uncertainty. The first theme could be 'COVID-19' from words, including \textit{lancet} and \textit{forearm}. \textit{Lancet} is a major medical journal. This could be related to the COVID-19. \textit{Forearm} could be associated with COVID-19 as it is a body part relevant to the vaccine. We secondly could infer ‘Innovation and Growth’. The words contained \textit{fang, bully, amenable, killing, granularity, koa}, and \textit{renter}. \textit{fang} referred to FANG, which is an acronym for major technology firms in the US. They were known for their volatile stock prices. Words like \textit{granularity, koa}, and \textit{renter} could be associated with a tech firm’s development. The word \textit{renter} indicated the concept of the sharing economy. A representative example of this could be cloud service. \textit{koa} represented a web framework for Node.js, which highlighted the back-end technology. Moreover, words such as \textit{bully, amenable}, and \textit{killing} could be interpreted as finance-relevant terms. \textit{Bully} could represent the bull market, and \textit{killing} is also referred to as slang for successfully dominating the finance market.

The following words could make an impact on low volatility:

\begin{align*}
\tilde{S_{-}}^{VOL} &= \textit{payday, swine, farming, brod, laryngoscope, moss, socialize} \\
               &\quad \textit{subway, malevolent, entrust, mop, zein, municipalization, awesome, roller} \\
\end{align*}

In fact, we could interpret the extracted words in various ways. One theme we could infer was ‘Environmental Issues’ from words, including \textit{swine, laryngoscope, farming, moss}, and \textit{subway}. \textit{Swine} could refer to swine flu. \textit{Laryngoscope} could be related to COVID-19. This flu and the pandemic could increase or decrease the volatility of the technology sector. Moreover, words such as \textit{farming, moss}, and \textit{subway} could be associated with modernisation or urbanisation for sustainable development. Or, they could be related to climate change.

\subsubsubsection{\textbf{The Sector Sentiment Model with Only-Risk-Factor} (\ref{fig:risk_qqq})}
\label{word_risk_qqq}

This model(\ref{fig:risk_qqq}) used a total of 9,030 words to predict its sentiment scores. The $\tilde{p}^{RET}$sentiment score model used 57\% of the total words(i.e. 5147 tokens) as 43\% was made up of neutral words. The $\tilde{p}^{VOL}$ sentiment score model employed 42\% of the total words(i.e. 3792 tokens) as 58\% were neutral words. 

The following words were extracted from the $\tilde{p}^{RET}$ model trained with the risk factor section.

\begin{align*}
\tilde{S_{+}}^{RET} &= \textit{wrongfully, mechanic, roe, th, kept, stance, determinable,} \\
               &\quad \textit{escheat, excellence, wake, artificially, magma, wane, ruble, explorer} \\
\end{align*}

These words affected the sector sentiment positively. The word set showed a similar theme compared to the previous one \hyperref[word_all_qqq]{check}. The first theme, ‘Innovation and Development,’ could be inferred from the words, including \textit{artificially}(AI), \textit{mechanic, excellence}, and \textit{explorer}. \textit{artificially} represents Artificial Intelligence(AI), which is a cornerstone of the fourth industrial revolution. \textit{Explorer} could be interpreted in the same context of maximizer, \textit{searcher} in the previous one \hyperref[word_all_qqq]{check}. The second theme was ‘Financial Health’. Words, such as \textit{roe, stance, escheat, wrongfully}, and \textit{ruble} could indicate the theme.\textit{roe} referred to ROE(i.e. Return on Equity). \textit{roe, stance}, and \textit{wrongfully} could symbolise firms’ financial condition. \textit{Escheat} could convey asset retention. Also, \textit{ruble} could indicate global market interactions.

The following words influenced the sector sentiment negatively. 

\begin{align*}
\tilde{S_{-}}^{RET} &= \textit{heat, map, biogen, density, ramification, covid, else, surviving,} \\
               &\quad \textit{ pirate, constellation, harmonize, chemotherapy, custody, entrust, kinase} \\
\end{align*}

There are a few words that could be interpreted as similar to the ‘Technological difficulty’ theme mentioned previously. The words included \textit{biogen, surviving, kinase, ramification}, and \textit{chemotherapy}. These words could specifically indicate biotechnology. \textit{biogen, kinase}, and \textit{chemotherapy} are terminology used in biotechnology. \textit{Ramification} and \textit{surviving} could be interpreted as a difficulty to develop biotechnology. Interestingly, this $\tilde{S_{-}}^{RET}$ model trained with the risk factor section extracted the word covid explicitly, which rapidly dropped the sector sentiment with the reduction of stock price.

The following words extracted in the risk factor sector could explain the rise of tech sector uncertainty. 

\begin{align*}
\tilde{S_{+}}^{VOL} &= \textit{substituted, seamless, programmatic, reimportation, pressing, prominently} \\
               &\quad \textit{clothing, anima, impropriety, spoof, kickback, endocrinologist, educator,} \\
               &\quad \textit{polymeric, erroneously}
\end{align*}

A few words, such as \textit{pressing, prominently}, and \textit{endocrinologist} could show the COVID-19 relevance. Also, \textit{Spoof} appeared again. It could symbolise cybersecurity, including \textit{impropriety, programmatic}, and \textit{erroneously}. Interestingly, \textit{clothing} and \textit{polymeric} were extracted. They could indicate wearable technology in health care.

The following words are the words extracted from the risk factor that contributed to reducing market uncertainty. 

\begin{align*}
\tilde{S_{-}}^{VOL} &= \textit{reuse, constriction, knew, mentor, intuit, sodium, sandy, } \\
               &\quad \textit{stall, cook, malevolent, snowstorm, maria, residue, mat, swine} \\
\end{align*}

\subsubsubsection{\textbf{The Top10 Sentiment Model with 10-K Filling} (\ref{fig:all_top10})}

\textbf{I am thinking how I can write this part shortly, or reduce the other parts of the most impactful word part If there are not big difference, go to Apeendix}
\begin{align*}
\tilde{S_{+}}^{RET} &= \textit{roadster, misconduct, musk, supercharger, agricultural, detriment, cluster, } \\
               &\quad \textit{owing, observe, rigor, roof, sedan, visible, ramping, dangerous} \\
\end{align*}
\begin{align*}
\tilde{S_{-}}^{RET} &= \textit{session, identifier, vista, portable, dram, snap, wrong,} \\
               &\quad \textit{snow, node, attendant, printed, peripheral, palm, composed, pride} \\
\end{align*}
\begin{align*}
\tilde{S_{+}}^{VOL} &= \textit{autonomous, ford, draft, berlin, thin, neural, audible,} \\
               &\quad \textit{accomplished, log, understatement, identifier, segregation, subjectivity, race, pilot} \\
\end{align*}
\begin{align*}
\tilde{S_{-}}^{VOL} &= \textit{estrain, club, deflation, voucher, ugh, explorer, insecure,} \\
               &\quad \textit{ticket, tester, murphy, fifty, seminar, resilience, hospital, mary} \\
\end{align*}

\subsubsubsection{\textbf{The Top10 Sentiment Model with Only-Risk-Factor} ({\ref{fig:risk_top10}})}

\begin{align*}
\tilde{S_{+}}^{RET} &= \textit{battery, solar, revolving, screen, musk, commensurate, secondary, } \\
               &\quad \textit{roadster, separately, dealer, motor, importation, installation, misstatement, convert} \\
\end{align*}
\begin{align*}
\tilde{S_{-}}^{RET} &= \textit{formation, wholly, family, entire, incorporated, dependence, nationwide, } \\
               &\quad \textit{misappropriate, wrong, sign, carrier, treat, subcontractor, entrust, remotely} \\
\end{align*}
\begin{align*}
\tilde{S_{+}}^{VOL} &= \textit{resale, eligible, tender, generating, according, circuit, virtual, } \\
               &\quad \textit{duplicate, franchise, deposit, air, though, representation, transact, attorney} \\
\end{align*}
\begin{align*}
\tilde{S_{-}}^{VOL} &= \textit{depot, host, sea, club, array, essential, membership, accountability,} \\
               &\quad \textit{subjective, surface, subscriber, azure, evidence, care, bing} \\
\end{align*}


\subsection{Nvidia Analysis}
\subsubsection{Nvidia Correlation Analysis}

\subsubsubsection{\textbf{Nvidia Sentiment Model with 10-K Fillings} (\ref{fig:all_nvidia})}

\begin{figure}[htpp]
\centering
\begin{minipage}{0.90\textwidth}
    \centering
    \includegraphics[width=1.0\linewidth]{ug/images/all_nvidia.jpeg}
    \caption{Nvidia Sentiment Model with 10-K Filling}
    \label{fig:all_nvidia}
\end{minipage}%
\hfill
\vspace{30pt} % Adjust the space as needed
\begin{minipage}{0.9\textwidth}

    \begin{minipage}[t]{0.9\textwidth}
    \centering
    \begin{tabular}{lcccc}
    \label{tab:all_nvidia_corr}
    $\tilde{p}$      & RET       & VOL       & LM        & Stock    \\ \hline
    RET    & 1  & ^{*}0.612  & ^{**}$-$0.832 & 0.234 \\
    VOL    & ^{*}0.612   & 1  & ^{*}$-$0.482 & 0.588  \\
    LM    & ^{**}$-$0.832 & ^{*}$-$0.482 & 1  & 0.182 \\
    Stock  & 0.234 & 0.588  & 0.182 & 1  \\ \hline
    \end{tabular}
    \medskip
    $\textit{Note}: ^{*}p$-$value<0.05, ^{**}p$-$value<0.005$
    \captionof{table}{Nvidia Sentiment Correlation with 10-K Filling}
    \end{minipage}

\end{minipage}
\end{figure}

\subsubsubsection{\textbf{Nvidia Sentiment Model with Only-Risk-Factor} (\ref{fig:risk_nvidia})}

\begin{figure}[htpp]
\centering
\begin{minipage}{0.90\textwidth}
    \centering
    \includegraphics[width=1.0\linewidth]{ug/images/risk_nvidia.jpeg}
    \caption{Nvidia Sentiment Model with Only-Risk-Factor}
    \label{fig:risk_nvidia}
\end{minipage}%
\hfill
\vspace{30pt} % Adjust the space as needed
\begin{minipage}{0.9\textwidth}

    \begin{minipage}[t]{0.9\textwidth}
    \centering
    \begin{tabular}{lcccc}
    \label{tab:risk_nvidia_corr}
    $\tilde{p}$      & RET       & VOL       & LM        & Stock    \\ \hline
    RET    & 1  & ^{*}0.490  & 0.057 & 0.228 \\
    VOL    & ^{*}0.490   & 1  & 0.004 & ^{**}0.712  \\
    LM    & 0.057 & 0.004 & 1  & $-$0.593 \\
    Stock  & 0.228 & ^{**}0.712  & $-$0.593 & 1  \\ \hline
    \end{tabular}
    \medskip
    $\textit{Note}: ^{*}p$-$value<0.05, ^{**}p$-$value<0.005$
    \captionof{table}{Nvidia Sentiment Correlation with Only-Risk-Factor}
    \end{minipage}

\end{minipage}
\end{figure}

\subsubsection{Nvidia Most Influential Words}

\subsubsubsection{\textbf{Nvidia Sentiment Model with 10-K Filling} (\ref{fig:all_nvidia})}

\begin{align*}
\tilde{S_{+}}^{RET} &= \textit{chain, weak, tender, russia, talent, ninth, thermal, conflict, } \\
               &\quad \textit{dispose, exclusion, strict, failing, connect, governmental, favor} \\
\end{align*}
\begin{align*}
\tilde{S_{-}}^{RET} &= \textit{redemption, grid, initiative, petition, eastern, big, retrospective, branded, } \\
               &\quad \textit{revolving, enactment, reducing, drone, joint, pursuit, broadcast} \\
\end{align*}
\begin{align*}
\tilde{S_{+}}^{VOL} &= \textit{tracing, starting, white, attempt, derive, turn, antitrust, pace, } \\
               &\quad \textit{retroactively, popularity, severe, subjectivity, accessible, percent, travel} \\
\end{align*}
\begin{align*}
\tilde{S_{-}}^{VOL} &= \textit{settle, dow, onto, occurrence, eventually, strike, unfair, allegation,} \\
               &\quad \textit{returned, grown, virus, programmer, fault, near} \\
\end{align*}


\subsubsubsection{\textbf{Nvidia Sentiment Model with Only-Risk-Factor} (\ref{fig:risk_nvidia})}

\begin{align*}
\tilde{S_{+}}^{RET} &= \textit{chain, conflict, mineral, group, implementation, card, item, weak, } \\
               &\quad \textit{antitrust, original, intangible, importance, pace, pose, thermal} \\
\end{align*}
\begin{align*}
\tilde{S_{-}}^{RET} &= \textit{directive, console, video, suit, three, go, pursuit, innovative, interested,} \\
               &\quad \textit{floating, initiative, discovered, elsewhere, generating, member} \\
\end{align*}
\begin{align*}
\tilde{S_{+}}^{VOL} &= \textit{responsible, procedure, social, restricted, included, severe, forecasting, } \\
               &\quad \textit{seeking, distribute, providing, full, begin, pose, mitigate, depending} \\
\end{align*}
\begin{align*}
\tilde{S_{-}}^{VOL} &= \textit{shareholder, hacker, fault, near, architecture, exercise, dilute, occurrence,} \\
               &\quad \textit{programmer, cessation, worm, geographic, miss, fourth, confidential} \\
\end{align*}

\section{Limitations}
The model can not currently capture meaningful phrase words as the model essentially calculates sentiment-charged words based on words. For instance, the model will extract the phrase word ‘chef executive officers(CEO)’ in a filling in a word base separately, and then evaluate whether each word can be sentiment-charged words concerning the dependent variables. In this word-based separation process, it loses the original meaning, which is CEO.

Industrial sentiment score prediction does not consider the allocation proportion of the QQQ portfolio. In the QQQ ETF fund, the top 10 firms take around 45\% allocation proportion of the total in 2023, and the rest of 90 firms take the rest of 55\% proportion. But, their portfolio is reconstructed annually. And, as such, to predict a robust industrial-level sentiment score, the scores should consider the portfolio allocation proportion. In our industrial sentiment trending analysis, however, all sentiment scores are equally considered as the portfolio rebalancing data is restricted to attain. For instance, the return-labelled sentiment score of Apple Inc. in 2023 is 0.006, whereas Amazon.com Inc.’s sentiment in 2023 is -0.009. We used these scores without weighting their portfolio proportion. In 2023, the QQQ allocated Apple at 9.22\% and Amazon at 4.83\%. For a robust sentiment score calculation, the sentiment score at the date should consider the allocation weight of the portfolio of the same date. To compensate for this, we used the Kalman filter to remove noisy signals for analysing a robust trend of the sector sentiments.

Our model can not adapt to the latest firms’ return or volatility on our model because our model calculation only works with the fillings released at the publication date. In other words, the predicted sentiment score is an annual data point at which the filling is released.

\chapter{Conclusions and Discussion}

\section{Final Reminder}

The body of your dissertation, before the references and any appendices,
\emph{must} finish by page~40. The introduction, after preliminary material,
should have started on page~1.

You may not change the dissertation format (e.g., reduce the font size, change
the margins, or reduce the line spacing from the default single spacing). Be
careful if you copy-paste packages into your document preamble from elsewhere.
Some \LaTeX{} packages, such as \texttt{fullpage} or \texttt{savetrees}, change
the margins of your document. Do not include them!

Over-length or incorrectly-formatted dissertations will not be accepted and you
would have to modify your dissertation and resubmit. You cannot assume we will
check your submission before the final deadline and if it requires resubmission
after the deadline to conform to the page and style requirements you will be
subject to the usual late penalties based on your final submission time.

% \bibliographystyle{plain}
% \bibliographystyle{abbrvnat}
\bibliographystyle{plainnat}
\bibliography{refs}


% You may delete everything from \appendix up to \end{document} if you don't need it.
\appendix

\chapter{10-K Report Form}
\cite{sec2010k, wiki10k}
\label{appendix_10-k}

\textbf{Part 1}

\textbf{Item 1: Business} - This section describes the business of the company: its main products or services, subsidiaries, and markets in which it operates. It may also contain recent events, competition, regulation, and labour problems. 

\textbf{Item 1A: Risk Factors} - This section provides risks and uncertainties, likely external effects, or possible failures that could affect their financial performance. Risk factors are generally enumerated based on their importance. 

\textbf{Item 1B: Unresolved Staff Comments} - This section offers an explanation of any issues raised by the SEC staff on the previous reports if these issues have not been resolved afterwards.

\textbf{Item 2:Properties} - This section only lays out the company’s significant physical properties, not intellectual or intangible property.

\textbf{Item 3: Legal Proceedings} - This section discloses any significant ongoing lawsuit or other legal proceeding. 

\textbf{Item 4} - [RESERVED] \\


\textbf{Part 2} 


\textbf{Item 5: Market for Registrant’s Common Equity, Related Stockholder Matters and Issuer Purchases of Equity Securities} - This section discloses their performance in the stock market, dividends, and repurchases of their own stocks. 

\textbf{Item 6} - [RESERVED]

\textbf{Item 7: Management’s Discussion and Analysis of Financial Condition and Results of Operations (MD&A)} - This section discusses the firm’s management for its financial performance, challenges, chances, and future outlook.

\textbf{Item 7A: Quantitative and Qualitative Disclosures about Market Risks} - This section shows the company’s exposure to market risks.

\textbf{Item 8: Financial Statement and Supplementary Data} - This section offers the audited financial statements. It contains the balance sheet, income statement, cash flow statement, and footnotes. 

\textbf{Item 9: Changes in and Disagreements with Accountants on Accounting and Financial Disclosure} - Companies address any alterations in or disputes with their accountants over financial reporting.

\textbf{Item 9A: Controls and Procedures} - This section details the company’s procedures for disclosure controls and its internal control mechanisms for financial reporting. 

\textbf{Item 9B: Other Information} - This section offers any additional information that does not align with the contents of other sections. \\


\textbf{Part 3} 


\textbf{Item 10: Directors, Executive Officers and Corporate Governance} - This section delves into the specifics of the company’s leadership, their respective roles, and the practices in place for corporate governance.

\textbf{Item 11: Executive Compensation} - This section addresses compensation policies and programmes, and the compensation of top executives. 

\textbf{Item 12: Security Ownership of Certain Beneficial Owners and Management and Related Stockholder Matters} - This section offers major shareholders’ ownership of the company’s stock as well as insiders.

\textbf{Item 13: Certain Relationships and Related Transactions, and Director Independence}- This section encompasses transactions involving directors, executives, and their affiliates, along with details regarding the independence of directors.

\textbf{Item 14: Principal Accountant Fees and Services} - The section details the fees charged by the company’s auditors for their services. \\
 

\textbf{Part 4} 


\textbf{Item 15: Exhibits, Financial Statement Schedules} - This section contains a list of the financial statements and exhibits.


\chapter{Invesco QQQ Trust, Series 1 Portfolio \cite{InvescoQQQ}}
\begin{longtable}
            \footnotesize
                \begin{tabular}[t]{|c|c|c|}
                    \hline
                    \textbf{Ticker} & \textbf{CIK} & \textbf{Weight} \\ \hline
AAPL & 320193 & 9.217 \\ \hline
MSFT & 789019 & 8.548 \\ \hline
AMZN & 1018724 & 4.867 \\ \hline
AVGO & 1730168 & 4.192 \\ \hline
META & 1326801 & 3.846 \\ \hline
TSLA & 1318605 & 3.79 \\ \hline
NVDA & 1045810 & 3.736 \\ \hline
GOOGL & 1652044 & 2.571 \\ \hline
GOOG & 1652044 & 2.51 \\ \hline
COST & 909832 & 2.316 \\ \hline
ADBE & 796343 & 2.16 \\ \hline
PEP & 77476 & 1.841 \\ \hline
AMD & 2488 & 1.837 \\ \hline
NFLX & 1065280 & 1.705 \\ \hline
INTC & 50863 & 1.688 \\ \hline
CSCO & 858877 & 1.62 \\ \hline
TMUS & 1283699 & 1.438 \\ \hline
CMCSA & 1166691 & 1.399 \\ \hline
INTU & 896878 & 1.387 \\ \hline
QCOM & 804328 & 1.284 \\ \hline
TXN & 97476 & 1.23 \\ \hline
AMGN & 318154 & 1.205 \\ \hline
AMAT & 6951 & 1.09 \\ \hline
HON & 773840 & 1.087 \\ \hline
BKNG & 1075531 & 0.987 \\ \hline
ISRG & 1035267 & 0.942 \\ \hline
SBUX & 829224 & 0.862 \\ \hline
VRTX & 875320 & 0.833 \\ \hline
LRCX & 707549 & 0.831 \\ \hline
GILD & 882095 & 0.79 \\ \hline
ADI & 6281 & 0.783 \\ \hline
MDLZ & 1103982 & 0.774 \\ \hline
PDD & 1737806 & 0.769 \\ \hline
MU & 723125 & 0.762 \\ \hline 
                \end{tabular}
                \hfill
                \begin{tabular}[t]{|c|c|c|}
                    \hline
                    \textbf{Ticker} & \textbf{CIK} & \textbf{Weight} \\ \hline
ADP & 8670 & 0.758 \\ \hline
PANW & 1327567 & 0.752 \\ \hline
REGN & 872589 & 0.722 \\ \hline
KLAC & 319201 & 0.637 \\ \hline
MELI & 1099590 & 0.632 \\ \hline
SNPS & 883241 & 0.627 \\ \hline
CDNS & 813672 & 0.593 \\ \hline
CSX & 277948 & 0.548 \\ \hline
PYPL & 1633917 & 0.535 \\ \hline
ASML & 937966 & 0.531 \\ \hline
MAR & 1048286 & 0.52 \\ \hline
CTAS & 723254 & 0.487 \\ \hline
LULU & 1397187 & 0.487 \\ \hline
ABNB & 1774585 & 0.478 \\ \hline
NXPI & 1413447 & 0.474 \\ \hline
MNST & 865752 & 0.468 \\ \hline
CRWD & 1535527 & 0.465 \\ \hline
ROP & 882835 & 0.46 \\ \hline
CHTR & 1091667 & 0.457 \\ \hline
WDAY & 1327811 & 0.454 \\ \hline
ORLY & 898173 & 0.442 \\ \hline
MRVL & 1835632 & 0.418 \\ \hline
ADSK & 769397 & 0.415 \\ \hline
PCAR & 75362 & 0.405 \\ \hline
MCHP & 827054 & 0.392 \\ \hline
DXCM & 1093557 & 0.378 \\ \hline
CPRT & 900075 & 0.373 \\ \hline
ROST & 745732 & 0.368 \\ \hline
IDXX & 874716 & 0.366 \\ \hline
KDP & 1418135 & 0.366 \\ \hline
FTNT & 1262039 & 0.364 \\ \hline
ODFL & 878927 & 0.36 \\ \hline
KHC & 1637459 & 0.355 \\ \hline
PAYX & 723531 & 0.344 \\ \hline
                \end{tabular}
                \hfill
                \begin{tabular}[t]{|c|c|c|}
                    \hline
                    \textbf{Ticker} & \textbf{CIK} & \textbf{Weight} \\ \hline
AEP & 4904 & 0.337 \\ \hline
AZN & 901832 & 0.307 \\ \hline
TEAM & 1650372 & 0.301 \\ \hline
BIIB & 875045 & 0.3 \\ \hline
CTSH & 1058290 & 0.3 \\ \hline
CEG & 1868275 & 0.298 \\ \hline
FAST & 815556 & 0.297 \\ \hline
DDOG & 1561550 & 0.296 \\ \hline
DASH & 1792789 & 0.294 \\ \hline
MRNA & 1682852 & 0.294 \\ \hline
EA & 712515 & 0.293 \\ \hline
ON & 1097864 & 0.292 \\ \hline
CSGP & 1057352 & 0.283 \\ \hline
GEHC & 1932393 & 0.282 \\ \hline
EXC & 1109357 & 0.28 \\ \hline
BKR & 1701605 & 0.277 \\ \hline
VRSK & 1442145 & 0.272 \\ \hline
XEL & 72903 & 0.27 \\ \hline
GFS & 1709048 & 0.269 \\ \hline
ZS & 1713683 & 0.264 \\ \hline
TTD & 1671933 & 0.26 \\ \hline
ANSS & 1013462 & 0.249 \\ \hline
CDW & 1402057 & 0.243 \\ \hline
DLTR & 935703 & 0.242 \\ \hline
CCEP & 1650107 & 0.24 \\ \hline
MDB & 1441816 & 0.236 \\ \hline
FANG & 1539838 & 0.226 \\ \hline
WBD & 1437107 & 0.222 \\ \hline
TTWO & 946581 & 0.218 \\ \hline
SPLK & 1353283 & 0.203 \\ \hline
WBA & 1618921 & 0.182 \\ \hline
ILMN & 1110803 & 0.177 \\ \hline
SIRI & 908937 & 0.168 \\ \hline
                \end{tabular}    
                \label{appendix_qqq}
                \caption{2023 QQQ Portfolio.}
                
\end{longtable}

\chapter{Formula}

\section{Pearson Correlation}
\label{pearson-formula}
\begin{equation}
    r = \frac{\sum (x_i - \bar{x})(y_i - \bar{y})}{\sqrt{\sum (x_i - \bar{x})^2 \sum (y_i - \bar{y})^2}},
\end{equation}

\begin{itemize}
    \item $r$ refers to the correlation coefficient.
    \item  $x_i$ and $y_i$ refer to the individual sample points for variables x and y, respectively. 
    \item $\bar{x}$ and $\bar{y}$ represent the mean values of the samples for x and y.
\end{itemize}
\cite{WikipediaPearson2024}

\chapter{Unfiltered Sentiment Prediction Scores}


\begin{figure}[ht]
\subsection{The Unfiltered Sector Sentiment Model with Only-Risk-Factor}
\label{appendix_risk_qqq}
  \centering
  \begin{minipage}{0.5\textwidth}
    \centering
    \includegraphics[width=\linewidth]{ug/images/risk_qqq_ret_filter.png}
    \caption{\small Unfiltered Sector ${p}^{RET}$}
    \label{fig:risk_qqq_ret_unfiltered}
  \end{minipage}%
  \begin{minipage}{0.5\textwidth}
    \centering
    \includegraphics[width=\linewidth]{ug/images/risk_qqq_vol_filter.png} 
    \caption{\small Unfiltered Sector ${p}^{VOL}$}
    \label{fig:risk_qqq_vol_unfiltered}
  \end{minipage}


    \begin{minipage}[t]{0.9\textwidth}
    \centering
    \begin{tabular}{lcccc}
    \label{tab:risk_top10_corr_unfiltered}
    $\hat{p}$      & RET       & VOL       & LM        & Stock    \\ \hline
    RET    & 1  & ^{**}$-$0.081  & 0.033 & ^{*}$-$0.071 \\
    VOL    & ^{**}$-$0.081   & 1  & ^{**}0.103  & 0.030 \\
    LM    & 0.033  & ^{**}0.103 & 1  & ^{**}$-$0.248 \\
    Stock  & ^{*}$-$0.071 & 0.030  & ^{**}$-$0.248 & 1  \\ \hline
    \end{tabular}
    \medskip
    $\textit{Note}: ^{*}p$-$value<0.05, ^{**}p$-$value<0.005$
    \captionof{table}{Unfiltered, The Top10 Sentiment Correlation with Only-Risk-Factor}
    \end{minipage}

\end{figure}


\begin{figure}[ht]
\subsection{The Unfiltered Top10 Sentiment Model with 10-K Filling}
\label{appendix_all_top10}
\begin{minipage}{0.90\textwidth}
  \centering
  \begin{minipage}{0.5\textwidth}
    \centering
    \includegraphics[width=\linewidth]{ug/images/all_top10_ret_filter.png}
    \caption{\small Unfiltered Top10 ${p}^{RET}$}
    \label{fig:all_top10_ret_unfiltered}
  \end{minipage}%
  \begin{minipage}{0.5\textwidth}
    \centering
    \includegraphics[width=\linewidth]{ug/images/all_top10_vol_filter.png} 
    \caption{\small Unfiltered Top10 ${p}^{VOL}$}
    \label{fig:all_top10_vol_unfiltered}
  \end{minipage}


    \begin{minipage}[t]{0.9\textwidth}
    \centering
    \begin{tabular}{lcccc}
    \label{tab:all_top10_corr_unfiltered}
    $\hat{p}$      & RET       & VOL       & LM        & Stock    \\ \hline
    RET    & 1  & 0.153  & 0.829 & ^{*}0.267 \\
    VOL    & 0.153   & 1  & ^{**}$-$0.349 & 0.117  \\
    LM    & 0.829 & ^{**}$-$0.349 & 1  & ^{*}$-$0.220 \\
    Stock  & ^{*}0.267 & 0.117  & ^{*}$-$0.220 & 1  \\ \hline
    \end{tabular}
    \medskip
    $\textit{Note}: ^{*}p$-$value<0.05, ^{**}p$-$value<0.005$
    \captionof{table}{Unfiltered, The Top10 Sentiment Model with 10-K Filling}
    \end{minipage}
    
\end{minipage}
\end{figure}



\begin{figure}[ht]
\subsection{The Unfiltered Top10 Sentiment Model with Only-Risk-Factor}
\label{appendix_risk_top10}
\begin{minipage}{0.90\textwidth}
  \centering
  \begin{minipage}{0.5\textwidth}
    \centering
    \includegraphics[width=\linewidth]{ug/images/risk_top10_ret_filter.png}
    \caption{\small Unfiltered Top10 ${p}^{RET}$}
    \label{fig:risk_top10_ret_unfiltered}
  \end{minipage}%
  \begin{minipage}{0.5\textwidth}
    \centering
    \includegraphics[width=\linewidth]{ug/images/risk_top10_vol_filter.png} 
    \caption{\small Unfiltered Top10 ${p}^{VOL}$}
    \label{fig:risk_top10_vol_unfiltered}
  \end{minipage}


    \begin{minipage}[t]{0.9\textwidth}
    \centering
    \begin{tabular}{lcccc}
    \label{tab:risk_top10_corr_unfiltered}
    $\hat{p}$      & RET       & VOL       & LM        & Stock    \\ \hline
    RET    & 1  & 0.037  & 0.202 &  0.105 \\
    VOL    & 0.037   & 1  & ^{**}0.414 & $-$0.021  \\
    LM    & 0.202 & ^{**}0.414 & 1  & ^{**}$-$0.447 \\
    Stock  &  0.105 & $-$0.021  & ^{**}$-$0.447 & 1  \\ \hline
    \end{tabular}
    \medskip
    $\textit{Note}: ^{*}p$-$value<0.05, ^{**}p$-$value<0.005$
    \captionof{table}{Unfiltered, The Top10 Sentiment Correlation with Only-Risk-Factor}
    \end{minipage}

\end{minipage}
\end{figure}

\chapter{Participants' consent form}

If you had human participants, include information about how consent was
gathered in an appendix, and point to it from the ethics declaration.
This information is often a copy of a consent form.


\end{document}
